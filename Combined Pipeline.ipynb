{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Combined Pipeline.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["9X8V2-sHN6M4","22hMLhdt7G0w","41c3kXlQoOS5","7kBvPCCLZJ60","ZeF54hjQZVJ4","3xVLu-nQiEpU","wwsi605FWP1H","1_jsR_x3Wg36"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"lFWjUrpBpV1k","colab_type":"code","outputId":"686a0157-3c04-4ac4-d71f-bd9e06986116","executionInfo":{"status":"ok","timestamp":1546097852973,"user_tz":-330,"elapsed":3254,"user":{"displayName":"Nithish Balachandar","photoUrl":"","userId":"11673492507565107492"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from __future__ import division\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# ! pip install --upgrade allennlp\n","\n","import os\n","import sys\n","import time\n","import nltk\n","import math\n","import random\n","import pickle\n","import zipfile\n","import numpy as np\n","import pandas as pd\n","# nltk.download('all')\n","import tensorflow as tf\n","from google.colab import drive\n","from collections import Counter\n","from IPython.display import SVG\n","from keras.utils import Sequence\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem.porter import PorterStemmer\n","from sklearn.preprocessing import MinMaxScaler\n","# from allennlp.commands.elmo import ElmoEmbedder\n","from nltk.translate.bleu_score import sentence_bleu\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from keras.optimizers import Adam, Adamax, Nadam, Adagrad\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot, plot_model\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"hxeqsOevAW6X","colab_type":"code","outputId":"69f7b251-242f-499e-a408-2654f37e80e9","executionInfo":{"status":"ok","timestamp":1546097927446,"user_tz":-330,"elapsed":77708,"user":{"displayName":"Nithish Balachandar","photoUrl":"","userId":"11673492507565107492"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["drive.mount(\"/content/gdrive\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"UWQ0EhXisKXU","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.test.gpu_device_name()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ELPdv9bbDftm","colab_type":"code","colab":{}},"cell_type":"code","source":["TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9X8V2-sHN6M4","colab_type":"text"},"cell_type":"markdown","source":["# Function Definitions"]},{"metadata":{"id":"h_eYYwGMrir3","colab_type":"code","colab":{}},"cell_type":"code","source":["porter = PorterStemmer()\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wdtNjKX6skrU","colab_type":"code","colab":{}},"cell_type":"code","source":["def clean(df,tasks,dtype,fname):\n","  column = 'query'\n","  #Lowercase conversion\n","  df[column] = df[column].apply(lambda x: x.lower())\n","  print(column+\": Converted to lowercase\")\n","  \n","  #Tokenization\n","  df[column] = df[column].apply(word_tokenize)\n","  df[column] = df[column].apply(lambda x: \" \".join(x))\n","  print(column+\": Tokenized\")\n","\n","  if('- ' in tasks):\n","    #Split a-b into a and b\n","    df[column] = df[column].str.replace('-',' ')\n","    print(column+\": - Replaced\")\n","  \n","  elif('-_' in tasks):\n","    #Split a-b into a and b\n","    df[column] = df[column].str.replace('-','_')\n","    print(column+\": - Replaced\")\n","  \n","  if('punct' in tasks):\n","    #Removing punctuations\n","    df[column] = df[column].str.replace('[^\\w\\s]',' ')\n","    print(column+\": Removed punctions \")\n","  \n","  if('num' in tasks):\n","    #Replacing numbers\n","    df[column] = df[column].str.replace('[0-9]','#')\n","    print(column+\": Replaced Numbers \")\n","  \n","  if('stop' in tasks):\n","    #Removing Stop Words\n","    df[column] = df[column].apply(lambda x: \" \".join([w for w in x.split() if w not in stop_words]))\n","    print(column+\": StopWords Removed\")\n","  \n","  if('lemma' in tasks):\n","    #Lemmatization - root words\n","    df[column] = df[column].apply(lambda x: \" \".join([lemmatizer.lemmatize(word,pos='v') for word in x.split()]))\n","    print(column+\": Root words Lemmatized\")\n","  \n","  column = 'response'\n","  #Lowercase conversion\n","  df[column] = df[column].apply(lambda x: \" \".join(k.lower() for k in x.split()))\n","  print(column+\": Converted to lowercase\")\n","  \n","  #Tokenization\n","  df[column] = df[column].apply(lambda x: \" \".join(word_tokenize(x)))\n","  print(column+\": Tokenized\")\n","  \n","  if('- ' in tasks):\n","    #Split a-b into a and b\n","    df[column] = df[column].str.replace('-',' ')\n","    print(column+\": - Replaced\")\n","  \n","  elif('-_' in tasks):\n","    #Split a-b into a and b\n","    df[column] = df[column].str.replace('-','_')\n","    print(column+\": - Replaced\")\n","  \n","  if('punct' in tasks):\n","    #Removing punctuations\n","    df[column] = df[column].str.replace('[^\\w\\s]',' ')\n","    print(column+\": Removed punctions \")\n","  \n","  if('num' in tasks):\n","    #Replacing numbers\n","    df[column] = df[column].str.replace('[0-9]','#')\n","    print(column+\": Replaced Numbers \")\n","  \n","  if('stop' in tasks):\n","    #Removing Stop Words\n","    df[column] = df[column].apply(lambda x: \" \".join([w for w in x.split() if w not in stop_words]))\n","    print(column+\": StopWords Removed\")\n","  \n","  if('lemma' in tasks):\n","    #Lemmatization - root words\n","    df[column] = df[column].apply(lambda x: \" \".join([lemmatizer.lemmatize(word,pos='v') for word in x.split()]))\n","    print(column+\": Root words Lemmatized\")\n","    \n","  df['Row_no'] = df.index  \n","  print(df.head())\n","  df.to_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\"/\"+fname+\".csv\",sep=',',index=None)\n","\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"JJmQWhIOysnk","colab_type":"code","colab":{}},"cell_type":"code","source":["archive = zipfile.ZipFile('gdrive/My Drive/Microsoft AI/Junk/Just For now/data.zip', 'r')\n","data = pd.read_csv(archive.open(\"data.tsv\"),sep='\\t',header=None)\n","data.columns = ['queryID','query','response','label','labelID']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8eX0o42LUtRd","colab_type":"code","colab":{}},"cell_type":"code","source":["data.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_wA7XdzrWyrT","colab_type":"code","colab":{}},"cell_type":"code","source":["[stop_words.remove(x) for x in ['the','what','why','how','when','who','which']]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mxUSNyS70ANo","colab_type":"code","colab":{}},"cell_type":"code","source":["tasks = ['- ','punct','lemma','num','stop']\n","clean(data,tasks,'Multi Hashed_stopwords_removed','clean_eval2_nostop')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AYxU700Ew-y1","colab_type":"code","colab":{}},"cell_type":"code","source":["def avg_word(sentence):\n","  words = sentence.split()\n","  return (sum(len(word) for word in words)/len(words))\n","\n","\n","def get_basic_features(df,ltype,dtype):\n","  \n","  column='query'\n","  df[column[0]+'_word_count'] = df[column].apply(lambda x: len(str(x).split(\" \")))\n","  print(column+\"Word Count Done\")\n","  df[column[0]+'_char_count'] = df[column].str.len()\n","  print(column+\"Char Count Done\")\n","  df[column[0]+'_avg_word'] = df[column].apply(lambda x: avg_word(x))\n","  print(column+\"Avg Word Length Done\")\n","  \n","  column='response'\n","  df[column[0]+'_word_count'] = df[column].apply(lambda x: len(str(x).split(\" \")))\n","  print(column+\"Word Count Done\")\n","  df[column[0]+'_char_count'] = df[column].str.len()\n","  print(column+\"Char Count Done\")\n","  df[column[0]+'_avg_word'] = df[column].apply(lambda x: avg_word(x))\n","  print(column+\"Avg Word Length Done\")\n","  \n","  df.to_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\\\n","            \"/Latest Features/\"+ltype+\"_Basic_features.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pVvJEIa2vU1f","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_cosine_sim(df,ltype,dtype):\n","  cosine_sim = []\n","  for i in range(0,df.shape[0],10):\n","    if(i%10000 == 0):\n","      print(i)\n","    sent = (df['query'][i], *(df['response'][i:i+10].T))\n","    text = [t for t in sent]\n","    vectorizer = TfidfVectorizer()\n","    vect = vectorizer.fit_transform(text).toarray()\n","    vectors = [t for t in vect]\n","    cosine_sim.extend(cosine_similarity(vectors)[0,1:].ravel())\n","  cosine_sim = pd.Series(cosine_sim).rename(\"Cosine\")\n","  cosine_sim.to_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\\\n","                    \"/Latest Features/\"+ltype+\"_Cosine_Similarity.csv\",sep=',',index=None,header=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VE0iyD27xKdJ","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_jaccard_sim(df,ltype,dtype):\n","  jac_sim = []\n","  for i in range(df.shape[0]):\n","    if(i%10000 == 0):\n","      print(i)\n","    a = set(df['query'][i].split())\n","    b = set(df['response'][i].split())\n","    c = a.intersection(b)\n","    jac_sim.append(float(len(c)) / (len(a) + len(b) - len(c)))\n","  jac_sim = pd.Series(jac_sim).rename(\"Jaccard\")\n","  print(jac_sim.shape)\n","  jac_sim.to_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\\\n","                 \"/Latest Features/\"+ltype+\"_Jaccard_Similarity.csv\",sep=',',index=None,header=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5JEZuE7kyjYQ","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_rouge_score(df,ltype,dtype):\n","  r1_recall = []\n","  r1_precision = []\n","  for i in range(df.shape[0]):\n","    if(i%10000 == 0):\n","      print(i)\n","    a = set(df['query'][i].split())\n","    b = set(df['response'][i].split())\n","    c = a.intersection(b)\n","    r1_recall.append(float (len(c)/len(a)))\n","    r1_precision.append(float (len(c)/len(b)))\n","  rouge_pd = pd.concat([pd.Series(r1_recall),pd.Series(r1_precision)],axis=1)\n","  rouge_pd.columns = ['Rouge R','Rouge P']\n","  print(rouge_pd.shape)\n","  rouge_pd.to_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\\\n","                  \"/Latest Features/\"+ltype+\"_Rouge_Score.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yUcx2q05zOhL","colab_type":"code","colab":{}},"cell_type":"code","source":["def TFIDFMatrix(docs):\n","  terms = []\n","  for i in range(len(docs)):\n","    words = docs[i].split()\n","    for w in words:\n","      if w not in terms:\n","        terms.append(w)\n","  \n","  tf = np.ndarray((len(terms),len(docs)))\n","  \n","  for i in range(len(terms)):\n","    for j in range(len(docs)):\n","      tf[i][j] = docs[j].split().count(terms[i])\n","      \n","  idf = np.array([np.log(1 + (float(len(docs))/np.count_nonzero(tf[i]))) for i in range(len(terms))])\n","  tf = 0.5 + 0.5*tf/tf.max(axis=0)  \n","  tfidf = np.array([tf[i]*idf[i] for i in range(len(terms))]).T\n","  return tfidf, terms\n","\n","def get_tfidf_score(df,ltype,dtype):\n","  scores = []\n","  for i in range(0,df.shape[0],10):\n","    \n","    if(i%10000 == 0):\n","      print(i)\n","    \n","    docs = df['response'][i:i+10].tolist()\n","    mat, terms = TFIDFMatrix(docs)\n","    mat = pd.DataFrame(mat, columns=terms)\n","    \n","    words = df['query'][i].split()\n","    tf = {}\n","    for w in words:\n","      if w not in tf.keys():\n","        tf[w] = 1\n","      else:\n","        tf[w] += 1\n","    mx = max(tf.values())\n","    new_row = np.zeros(mat.shape[1])\n","    for k in tf.keys():\n","      tf[k] = 0.5+0.5*tf[k]/mx\n","      try:\n","        new_row[mat.columns.tolist().index(k)] = tf[k]\n","      except:\n","        pass\n","    mat = pd.DataFrame(np.vstack((new_row,mat.values)),columns=terms)\n","    scores.extend(cosine_similarity(mat.values)[0,1:])\n","  scores = pd.Series(scores).rename(\"Tfidf\")\n","  print(scores.shape)\n","  scores.to_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\\\n","                \"/Latest Features/\"+ltype+\"_Tfidf_Score.csv\",sep=',',index=None,header=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Lira57cq0OiA","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_bleu_score(df,ltype,dtype):\n","  bleu_score = []\n","  for i in range(df.shape[0]):\n","    if(i%10000 == 0):\n","      print(i)\n","    bleu_score.append([sentence_bleu([df['response'][i].split()],df['query'][i].split(),weights=(1,0)), sentence_bleu([df['response'][i].split()],df['query'][i].split(),weights=(1,0))])\n","  bleu_score = pd.DataFrame(bleu_score)\n","  bleu_score.columns = ['BLEU 1','BLEU 2']\n","  print(bleu_score.shape)\n","  bleu_score.to_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\\\n","                    \"/Latest Features/\"+ltype+\"_BLEU_Score.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cSiv5GF1XlhM","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_bm25_score(ltype,dtype):\n","  bm_data = pd.read_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\"/Latest Features/\"+ltype+\"_BM25.tsv\",sep='\\t',header=None)\n","  bm = []\n","  for i in range(len(bm_data)):\n","    bm.extend(bm_data.iloc[i,1:].tolist())\n","  bm = pd.Series(bm).rename(\"BM25\")\n","  print(bm.shape)\n","  bm.to_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\\\n","            \"/Latest Features/\"+ltype+\"_BM25.csv\",sep=',',index=None,header=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vu0CKHKWjH_z","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_dssm_score(ltype,dtype):\n","  with open(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\"/Latest Features/DSSM/\"+ltype+\"_dssm.txt\") as f:\n","    dvalues = f.readlines()\n","  with open(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\"/Latest Features/DSSM/\"+ltype+\"_cdssm.txt\") as f:\n","    cvalues = f.readlines()\n","  dvalues = [float(val.split(\"\\n\")[0]) for val in dvalues]\n","  cvalues = [float(val.split(\"\\n\")[0]) for val in cvalues]\n","  if(ltype=='Train'):\n","    dssm = pd.concat([pd.Series(dvalues[1:]),pd.Series(cvalues[1:])],axis=1)\n","  else:\n","    dssm = pd.concat([pd.Series(dvalues),pd.Series(cvalues)],axis=1)\n","  dssm.columns = ['DSSM','SDSSM']\n","  print(dssm.shape)\n","  dssm.to_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\\\n","              \"/Latest Features/\"+ltype+\"_DSSM.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"22hMLhdt7G0w","colab_type":"text"},"cell_type":"markdown","source":["# ELMO Embeddings"]},{"metadata":{"id":"D2nUT_GMD9fR","colab_type":"code","colab":{}},"cell_type":"code","source":["! wget \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n","! wget \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"6dxxJ2JNh7OQ","colab_type":"code","colab":{}},"cell_type":"code","source":["options_file = \"./elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n","weight_file = \"./elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"T4kZmIJll19A","colab_type":"code","colab":{}},"cell_type":"code","source":["elmo = ElmoEmbedder(options_file, weight_file, 0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MYsaOkISmrdw","colab_type":"code","colab":{}},"cell_type":"code","source":["def padding_sent(ls,max_len,pad_val):\n","  for i in range(len(ls),max_len):\n","    ls.append(pad_val)\n","  return ls"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eUNyF5flmG6Y","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.decomposition import PCA\n","pca = PCA(n_components=100)\n","\n","def get_elmo_embed(elmo,data,fname,dname):\n","  vecs = []\n","  for i in range(0,len(data),100):\n","    if(i%10000 == 0 and i!= 0):\n","      print(i)\n","      pickle.dump(vecs,open(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dname+\"/ELMO Embeddings/\"+fname+\"_\"+str(i)+\".pkl\",\"wb\"))\n","      del vecs\n","      vecs = []\n","    tmp = np.float16(np.array(elmo.embed_batch(data[i:i+100]))[:,2,:,:])\n","    red_tmp = []\n","    for j in range(tmp.shape[0]):\n","      red_tmp.append(np.float16(pca.fit_transform(tmp[j])))\n","    vecs.extend(np.array(red_tmp,dtype=np.float16))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YQlq3aTO9Y1Y","colab_type":"code","colab":{}},"cell_type":"code","source":["qry = pickle.load(open(\"gdrive/My Drive/Microsoft AI/Finalised Version/Data_for_ELMO/post_padded_15L_query.pkl\",\"rb\"))\n","get_elmo_embed(elmo,data,'query','query_elmo_embeddings','Data_for_ELMO',q_max_len,pad_val)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hlIZqJCp93QG","colab_type":"code","colab":{}},"cell_type":"code","source":["rsp1 = pickle.load(open(\"gdrive/My Drive/Microsoft AI/Finalised Version/Data_for_ELMO/post_padded_5L_response.pkl\",\"rb\"))\n","get_elmo_embed(elmo,rsp1,'response_elmo_embeddings_5L','Data_for_ELMO')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jmB8Jy0vJRb8","colab_type":"code","colab":{}},"cell_type":"code","source":["rsp2 = pickle.load(open(\"gdrive/My Drive/Microsoft AI/Finalised Version/Data_for_ELMO/post_padded_10L_response.pkl\",\"rb\"))\n","get_elmo_embed(elmo,rsp2,'response_elmo_embeddings_10L','Data_for_ELMO')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4_pN0AqRJR8i","colab_type":"code","colab":{}},"cell_type":"code","source":["rsp3 = pickle.load(open(\"gdrive/My Drive/Microsoft AI/Finalised Version/Data_for_ELMO/post_padded_15L_response.pkl\",\"rb\"))\n","get_elmo_embed(elmo,rsp3,'response','response_elmo_embeddings_15L','Data_for_ELMO',r_max_len,pad_val)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"41c3kXlQoOS5","colab_type":"text"},"cell_type":"markdown","source":["# Data Loading"]},{"metadata":{"id":"JRW1G_Br1HtF","colab_type":"code","colab":{}},"cell_type":"code","source":["data = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Finalised Version/Data_for_ELMO/clean_train_elmo.csv\",sep=',')\n","data.columns = ['queryID','query','response','label','labelID']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vVMBxdIBnFm7","colab_type":"code","colab":{}},"cell_type":"code","source":["data.isnull().sum()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uQGWgMLgZ0_P","colab_type":"code","colab":{}},"cell_type":"code","source":["data2 = pd.read_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/Hashed/Latest Data/clean_test_data_#.csv\",sep=',')\n","data2.columns = ['queryID','query','response','labelID']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7kBvPCCLZJ60","colab_type":"text"},"cell_type":"markdown","source":["## Train Data Features Extraction"]},{"metadata":{"id":"xS_js-5bonEV","colab_type":"code","colab":{}},"cell_type":"code","source":["get_cosine_sim(data,'Train','Hashed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ehLNuh8go6Vb","colab_type":"code","colab":{}},"cell_type":"code","source":["get_jaccard_sim(data,'Train','Hashed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KGlxMv3do_r7","colab_type":"code","colab":{}},"cell_type":"code","source":["get_rouge_score(data,'Train','Hashed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9ONaaf2bpCRb","colab_type":"code","colab":{}},"cell_type":"code","source":["get_tfidf_score(data,'Train','Hashed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"imIeAcjipEFa","colab_type":"code","colab":{}},"cell_type":"code","source":["get_bleu_score(data,'Train','Hashed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"G6QqS_Z_Zu2k","colab_type":"code","colab":{}},"cell_type":"code","source":["get_bm25_score(\"Train\",\"Hashed\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XrADQN3qmiGd","colab_type":"code","colab":{}},"cell_type":"code","source":["get_dssm_score('Train','Hashed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bU6I31IvysJ_","colab_type":"code","colab":{}},"cell_type":"code","source":["get_basic_features(data,'Train','Hashed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZeF54hjQZVJ4","colab_type":"text"},"cell_type":"markdown","source":["## Test Data Features Extraction"]},{"metadata":{"id":"5VXwQLxlZs3n","colab_type":"code","colab":{}},"cell_type":"code","source":["get_cosine_sim(data2,'Test','Hashed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oUkrIUNZZiXY","colab_type":"code","colab":{}},"cell_type":"code","source":["get_jaccard_sim(data2,'Test','Hashed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ECUU47GzZmLE","colab_type":"code","colab":{}},"cell_type":"code","source":["get_rouge_score(data2,'Test','Hashed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RwMfbXtOZo18","colab_type":"code","colab":{}},"cell_type":"code","source":["get_tfidf_score(data2,'Test','Hashed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uxwPLKXxZk4a","colab_type":"code","colab":{}},"cell_type":"code","source":["get_bleu_score(data2,'Test','Hashed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j9NXajMVhqcp","colab_type":"code","colab":{}},"cell_type":"code","source":["get_bm25_score(\"Test\",\"Hashed\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dfbEuMBImo21","colab_type":"code","colab":{}},"cell_type":"code","source":["get_dssm_score('Test','Hashed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Cfy3Ux3G1Gcf","colab_type":"code","colab":{}},"cell_type":"code","source":["get_basic_features(data2,\"Test\",\"Hashed\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3xVLu-nQiEpU","colab_type":"text"},"cell_type":"markdown","source":["## Merge Features"]},{"metadata":{"id":"6PZCltFJoGWv","colab_type":"code","colab":{}},"cell_type":"code","source":["def merge_features(data,ltype, dtype):\n","  path = os.path.join(\"gdrive/My Drive/Microsoft AI/Finalised Version\",dtype,\"Latest Features\",ltype+\"_\")\n","  \n","  s1 = pd.read_csv(path+\"QueryID.csv\",sep=',',header=None).reset_index(drop=True)\n","  s2 = pd.read_csv(path+\"Cosine_Similarity.csv\",sep=',',header=None).reset_index(drop=True)\n","  s3 = pd.read_csv(path+\"Jaccard_Similarity.csv\",sep=',',header=None).reset_index(drop=True)\n","  s4 = pd.read_csv(path+\"BM25.csv\",sep=',',header=None).reset_index(drop=True)\n","  s5 = pd.read_csv(path+\"Tfidf_Score.csv\",sep=',',header=None).reset_index(drop=True)\n","  s6 = pd.read_csv(path+\"Rouge_Score.csv\",sep=',').reset_index(drop=True)\n","  s7 = pd.read_csv(path+\"BLEU_Score.csv\",sep=',').reset_index(drop=True)\n","  s8 = pd.read_csv(path+\"DSSM.csv\",sep=',').reset_index(drop=True)\n","  if(ltype == 'Train'):\n","    s9 = data['label'].reset_index(drop=True)\n","    s10 = pd.read_csv(path+\"Basic_features.csv\",sep=',',low_memory=False).drop(columns=['queryID','query','response','label','labelID']).reset_index(drop=True)\n","    all_feat = pd.concat([s1,s2,s3,s4,s5,s6,s7,s8,s9,s10],axis=1)\n","    all_feat.columns =  ['qid','Cosine','Jaccard','BM25','Tfidf','Rouge P','Rouge R','BLEU 1','BLEU 2','DSSM','CDSSM','label','q_word_count','q_char_count','q_avg_word','r_word_count','r_char_count','r_avg_word']\n","    all_feat.to_csv(\"gdrive/My Drive/Microsoft AI/Finalised Version/All_\"+ltype+\"Features.csv\",sep=',',index=None)\n","  else:\n","    s9 = pd.read_csv(path+\"Basic_features.csv\",sep=',',low_memory=False).drop(columns=['queryID','query','response','labelID']).reset_index(drop=True)\n","    all_feat = pd.concat([s1,s2,s3,s4,s5,s6,s7,s8,s9],axis=1)\n","    all_feat.columns =  ['qid','Cosine','Jaccard','BM25','Tfidf','Rouge P','Rouge R','BLEU 1','BLEU 2','DSSM','CDSSM','q_word_count','q_char_count','q_avg_word','r_word_count','r_char_count','r_avg_word']\n","    all_feat.to_csv(\"gdrive/My Drive/Microsoft AI/Finalised Version/All_\"+ltype+\"Features.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1AlbX_O0xl6y","colab_type":"code","colab":{}},"cell_type":"code","source":["merge_features(data,'Train','Hashed')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"27NjEGcz519T","colab_type":"code","colab":{}},"cell_type":"code","source":["merge_features(data2,\"Test\",\"Hashed\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"28wH_tw3VF9-","colab_type":"text"},"cell_type":"markdown","source":["# Model and Hyper parameters"]},{"metadata":{"id":"ONUOQXZEW4qo","colab_type":"code","colab":{}},"cell_type":"code","source":["def data_gen(com_csv,sr=0.7,ratio=3):\n","  features = ['Cosine', 'Jaccard', 'BM25', 'Tfidf', 'Rouge P', 'Rouge R', 'BLEU 1',\n","            'BLEU 2', 'DSSM', 'CDSSM', 'q_word_count', 'q_char_count', 'q_avg_word',\n","            'r_word_count', 'r_char_count', 'r_avg_word']\n","  \n","  combined_data = pd.read_csv(com_csv,sep=',')\n","  \n","  scaler = MinMaxScaler()\n","  \n","  split = int(10*((combined_data.shape[0]*sr)//10))\n","  print(split)\n","  \n","  train_data, test_data = combined_data.iloc[:split].reset_index( drop = True), combined_data.iloc[split:].reset_index( drop = True)\n","  \n","  tr_ind = train_data[train_data['label']==1].index\n","  fl_ind = train_data[train_data['label']==0].index\n","\n","  tr_data, fl_data = train_data.iloc[tr_ind].reset_index( drop = True), train_data.iloc[fl_ind].reset_index( drop = True)\n","\n","  tr_ind = random.sample(range(0,tr_data.shape[0]),tr_data.shape[0])\n","  fl_ind = random.sample(range(0,fl_data.shape[0]),tr_data.shape[0]*ratio)\n","\n","  train_data = pd.concat([tr_data.iloc[tr_ind],fl_data.iloc[fl_ind]],axis=0).sample(frac=1).reset_index( drop = True)\n","\n","  train_queryID, test_queryID = train_data.values[:,70], test_data.values[:,70]\n","  train_labelID, test_labelID = train_data.values[:,-1], test_data.values[:,-1]\n","#   train_f_data, test_f_data = train_data.loc[:,features].values, test_data.loc[:,features].values\n","  train_f_data, test_f_data = scaler.fit_transform(train_data.loc[:,features].values), scaler.fit_transform(test_data.loc[:,features].values)\n","  train_q_embed, test_q_embed = train_data.values[:,:30], test_data.values[:,:30]\n","  train_r_embed, test_r_embed = train_data.values[:,30:60], test_data.values[:,30:60]\n","  train_labels, test_labels = train_data.values[:,71], test_data.values[:,71]\n","\n","  return features,train_f_data,train_q_embed,train_r_embed,train_labels,test_f_data,test_q_embed,test_r_embed,test_labels,test_labelID,test_queryID"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4dcauMuRwo4j","colab_type":"code","outputId":"54fdf2c9-e8de-469a-b9b4-4b1b7d06ac6d","executionInfo":{"status":"ok","timestamp":1546105152375,"user_tz":-330,"elapsed":88295,"user":{"displayName":"Nithish Balachandar","photoUrl":"","userId":"11673492507565107492"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["features,train_f_data,train_q_embed,train_r_embed,train_labels,test_f_data,test_q_embed,test_r_embed,test_labels,test_labelID,test_queryID\\\n","= data_gen(com_csv=\"gdrive/My Drive/Microsoft AI/Finalised Version/Combined_Data.csv\",sr=0.8,ratio=2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1599990\n"],"name":"stdout"}]},{"metadata":{"id":"s_irxGBszoK8","colab_type":"code","colab":{}},"cell_type":"code","source":["features = ['Cosine','Jaccard','BM25','Tfidf','Rouge P','Rouge R','BLEU 1','BLEU 2','DSSM','CDSSM',\n","            'q_word_count','q_char_count','q_avg_word','r_word_count','r_char_count','r_avg_word']\n","\n","data = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Finalised Version/All_TrainFeatures.csv\")\n","split = int(10*((data.shape[0]*0.7)//10))\n","print(split)\n","\n","train_data, test_data = data.iloc[:split]\n","\n","tr_data = data[data['label'] == 1].reset_index(drop=True)\n","fl_data = data[data['label'] == 0].reset_index(drop=True)\n","tr_ind = random.sample(range(0,tr_data.shape[0]),tr_data.shape[0])\n","fl_ind = random.sample(range(0,fl_data.shape[0]),tr_data.shape[0]*3)\n","df = pd.concat([tr_data.iloc[tr_ind],fl_data.iloc[fl_ind]],axis=0).sample(frac=1).reset_index( drop = True)\n","\n","y = df['label']\n","x = df[features]\n","print(len(features))\n","\n","# x = (x - x.min()) / (x.max() - x.min())\n","x = (x - df.mean()) / df.std()\n","x_train, x_test = x.values[:split,:], x.values[split:,:]\n","y_train, y_test = y.values[:split], y.values[split:]\n","x_train.shape, y_train.shape, x_test.shape, y_test.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wwsi605FWP1H","colab_type":"text"},"cell_type":"markdown","source":["## Hyper Parameter Tuning"]},{"metadata":{"id":"0cHU2sbzVlvX","colab_type":"code","colab":{}},"cell_type":"code","source":["hyperparams = {\n","    \"trees\":{\n","        \"criterions\" : [\"gini\",\"entropy\"],\n","        \"max_depth\" : [3,4,5,6,7,8,9,10],\n","        \"max_features\": [\"log2\",\"sqrt\",\"auto\"], \n","        \"n_estimators\": [20,40,60,80,100]}, \n","    \"gbc\":{\n","        \"loss\" : [\"deviance\",\"exponential\"],\n","        \"learning_rate\" : [0.1,0.5,0.8,1.0],\n","        \"n_estimators\" : [20,40,60,80,100],\n","        \"max_features\" : [\"log2\",\"sqrt\",\"auto\"],\n","        \"max_depth\" : [3,4,5,6,7,8,9,10]},\n","    \"nn\":{\n","        \"learning_rate\" : [0.1,0.01,0.05,0.001,0.005],\n","        \"optim\" : [\"Adam\",\"Adamax\",\"Adagrad\",\"Nadam\"], \n","        \"activation\": [\"tanh\",\"sigmoid\",\"relu\",\"linear\"]}\n","}\n","\n","results = {}\n","results['RFC'] = {}\n","\n","for cr in hyperparams[\"trees\"][\"criterions\"]:\n","    results['RFC'][str(cr)] = {}\n","    for dep in hyperparams[\"trees\"][\"max_depth\"]:\n","        results['RFC'][str(cr)][str(dep)] = {}\n","        for feat in hyperparams[\"trees\"][\"max_features\"]:\n","            results['RFC'][str(cr)][str(dep)][str(feat)] = {}\n","            for est in hyperparams[\"trees\"][\"n_estimators\"]:\n","                results['RFC'][str(cr)][str(dep)][str(feat)][str(est)] = {}\n","                print(\"Hyperparameters used\")\n","                print(\"Criterion : \",cr)\n","                print(\"Maximum Depth : \",dep)\n","                print(\"Maximum Features : \",feat)\n","                print(\"Number of Estimators : \",est)\n","                model = RandomForestClassifier(n_estimators=est,\n","                                              criterion=cr,\n","                                              max_features=feat,\n","                                              max_depth=dep,\n","                                              verbose=3,\n","                                              class_weight=\"balanced\")\n","                \n","                model.fit(x_train,y_train)\n","                y_pred = model.predict(x_test)\n","                imp = model.feature_importances_\n","                score = model.score(x_test,y_test)\n","                print(score)\n","                results['RFC'][str(cr)][str(dep)][str(feat)][str(est)][\"score\"] = score\n","                \n","                f = pd.DataFrame(np.column_stack((features,imp)))\n","                f.columns = ['feature','importance']\n","                print(f.sort_values(by=['importance'],ascending=False))\n","                results['RFC'][str(cr)][str(dep)][str(feat)][str(est)][\"f_imp\"] = f\n","\n","results['ETC'] = {}\n","\n","for cr in hyperparams[\"trees\"][\"criterions\"]:\n","    results['ETC'][str(cr)] = {}\n","    for dep in hyperparams[\"trees\"][\"max_depth\"]:\n","        results['ETC'][str(cr)][str(dep)] = {}\n","        for feat in hyperparams[\"trees\"][\"max_features\"]:\n","            results['ETC'][str(cr)][str(dep)][str(feat)] = {}\n","            for est in hyperparams[\"trees\"][\"n_estimators\"]:\n","                results['ETC'][str(cr)][str(dep)][str(feat)][str(est)] = {}\n","                print(\"Hyperparameters used\")\n","                print(\"Criterion : \",cr)\n","                print(\"Maximum Depth : \",dep)\n","                print(\"Maximum Features : \",feat)\n","                print(\"Number of Estimators : \",est)\n","                model = ExtraTreesClassifier(n_estimators=est,\n","                                              criterion=cr,\n","                                              max_features=feat,\n","                                              max_depth=dep,\n","                                              verbose=3,\n","                                              warm_start=True,\n","                                              class_weight=\"balanced\")\n","                \n","                model.fit(x_train,y_train)\n","                y_pred = model.predict(x_test)\n","                imp = model.feature_importances_\n","                score = model.score(x_test,y_test)\n","                print(score)\n","                results['ETC'][str(cr)][str(dep)][str(feat)][str(est)][\"score\"] = score\n","                \n","                f = pd.DataFrame(np.column_stack((features,imp)))\n","                f.columns = ['feature','importance']\n","                print(f.sort_values(by=['importance'],ascending=False))\n","                results['ETC'][str(cr)][str(dep)][str(feat)][str(est)][\"f_imp\"] = f\n","\n","results['GBC'] = {}\n","\n","for ls in hyperparams[\"gbc\"][\"loss\"]:\n","    results['GBC'][str(ls)] = {}\n","    for lr in hyperparams[\"gbc\"][\"learning_rate\"]:\n","        results['GBC'][str(ls)][str(lr)] = {}\n","        for est in hyperparams[\"gbc\"][\"n_estimators\"]:\n","            results['GBC'][str(ls)][str(lr)][str(est)] = {}\n","            for feat in hyperparams[\"gbc\"][\"max_features\"]:\n","                results['GBC'][str(ls)][str(lr)][str(est)][str(feat)] = {}\n","                for dep in hyperparams[\"gbc\"][\"max_depth\"]:\n","                    results['GBC'][str(ls)][str(lr)][str(est)][str(feat)][str(dep)] = {}\n","                    print(\"Hyperparameters used\")\n","                    print(\"Loss : \",ls)\n","                    print(\"Learning rate : \",lr)\n","                    print(\"Number of Estimators : \",est)\n","                    print(\"Maximum Features : \",feat)\n","                    print(\"Maximum Depth : \",dep)\n","                    model = GradientBoostingClassifier(loss=ls,\n","                                                       learning_rate=lr,\n","                                                       n_estimators=est,\n","                                                       max_features=feat,\n","                                                       max_depth=dep,\n","                                                       verbose=3)\n","\n","                    model.fit(x_train,y_train)\n","                    y_pred = model.predict(x_test)\n","                    imp = model.feature_importances_\n","                    score = model.score(x_test,y_test)\n","                    print(score)\n","                    results['GBC'][str(ls)][str(lr)][str(est)][str(feat)][str(dep)][\"score\"] = score\n","\n","                    f = pd.DataFrame(np.column_stack((features,imp)))\n","                    f.columns = ['feature','importance']\n","                    print(f.sort_values(by=['importance'],ascending=False))\n","                    results['GBC'][str(ls)][str(lr)][str(est)][str(feat)][str(dep)][\"f_imp\"] = f\n","\n","# tree models tabulation\n","columns = [\"model type\",\"criterion\",\"learning rate\",\"max features\",\"max depth\",\"# of estimators\",\"score\"]\n","l = []\n","for mdl in results.keys():\n","    if(mdl != \"GBC\"):\n","        for crt in results[mdl].keys():\n","            for mfeat in results[mdl][crt].keys():\n","                for mdep in results[mdl][crt][mfeat].keys():\n","                    for nest in results[mdl][crt][mfeat][mdep].keys():\n","                        score = results[mdl][crt][mfeat][mdep][nest][\"score\"]\n","                        l.append([mdl,crt,None,mfeat,mdep,nest,score])\n","    else:\n","        for crt in results[mdl].keys():\n","            for lr in results[mdl][crt].keys():\n","                for nest in results[mdl][crt][lr].keys():\n","                    for mfeat in results[mdl][crt][lr][nest].keys():\n","                        for mdep in results[mdl][crt][lr][nest][mfeat].keys():\n","                            score = results[mdl][crt][lr][nest][mfeat][mdep][\"score\"]\n","                            l.append([mdl,crt,lr,mfeat,mdep,nest,score])\n","\n","res = pd.DataFrame(l,columns=columns)\n","res = res.sort_values(by=['score'],ascending=False).reset_index(drop=True)\n","\n","res.to_csv(\"./Tree_Hyper.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jLXLWx_4WVYj","colab_type":"code","colab":{}},"cell_type":"code","source":["results['nn'] = {}\n","for optim in hyperparams['nn']['optim']:\n","    results['nn'][str(optim)] = {}\n","    for lr in hyperparams['nn']['learning_rate']:\n","        results['nn'][str(optim)][str(lr)] = {}\n","        for act in hyperparams['nn']['activation']:\n","            print(\"Hyperparameter used\")\n","            print(\"Optimizer : \",optim)\n","            print(\"Learning rate : \",lr)\n","            print(\"Activation : \",act)\n","            results['nn'][str(optim)][str(lr)][str(act)] = {}\n","            inp = L.Input(shape=(len(features),))\n","            x = L.Dense(512, activation=act)(inp)\n","            x = L.Dense(256, activation=act)(x)\n","            x = L.Dense(128, activation=act)(x)\n","            x = L.Dense(128, activation=act)(x)\n","            x = L.Dense(64, activation=act)(x)\n","            out = L.Dense(1, activation=\"sigmoid\")(x)\n","            \n","            model = M.Model(inputs=[inp],outputs=[out])\n","            model.summary()\n","            if optim == \"Adam\":\n","                o = Adam(lr)\n","            elif optim == \"Adagrad\":\n","                o = Adagrad(lr)\n","            elif optim == \"Adamax\":\n","                o = Adamax(lr)\n","            elif optim == \"Nadam\":\n","                o = Nadam(lr)\n","            model.compile(o,loss='binary_crossentropy',metrics=['accuracy'])\n","            \n","            model.fit(x_train,y_train,epochs=10)\n","            score = model.evaluate(x_test,y_test)\n","            y_pred = model.predict(x_test)\n","            \n","            results['nn'][str(optim)][str(lr)][str(act)][\"score\"] = score\n","\n","# tree models tabulation\n","columns = [\"optimizer\",\"learning rate\",\"activation\",\"score\"]\n","l = []\n","for mdl in results.keys():\n","    if(mdl == \"nn\"):\n","        for optim in results[mdl].keys():\n","            for lr in results[mdl][optim].keys():\n","                for act in results[mdl][optim][lr].keys():\n","                    score = results[mdl][optim][lr][act][\"score\"]\n","                    l.append([optim,lr,act,score])\n","\n","res = pd.DataFrame(l,columns=columns)\n","res[\"loss\"] = [s[0] for s in res[\"score\"]]\n","res[\"score\"] = [s[1] for s in res[\"score\"]]\n","\n","res = res.sort_values(by=['score'],ascending=False).reset_index(drop=True)\n","res.to_csv(\"NN_Hyper.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GQiyTHLBWTZC","colab_type":"text"},"cell_type":"markdown","source":["## Model Training"]},{"metadata":{"id":"SleStFzdVibe","colab_type":"code","colab":{}},"cell_type":"code","source":["#model type,criterion,learning rate,max features,max depth,# of estimators,score\n","#GBC,exponential,0.1,log2,7,100,0.900060606060606\n","#RFC,gini,,10,auto,80,0.6947878787878788\n","ls = \"exponential\"\n","lr = 0.1\n","est = 100\n","feat = \"log2\"\n","dep = 7\n","\n","# model1 = RandomForestClassifier(n_estimators=est,criterion=cr,max_features=feat,max_depth=dep,verbose=3,class_weight=\"balanced_subsample\")\n","model1 = GradientBoostingClassifier(loss=ls, learning_rate=lr,n_estimators=est,max_features=feat,max_depth=dep,verbose=3)\n","# model1 = ExtraTreesClassifier(n_estimators=100, criterion='gini', verbose=0,n_jobs=-1, max_features='auto', min_samples_split=3, min_samples_leaf=20)\n","\n","model1.fit(x_train,y_train)\n","\n","y_pred = model1.predict(x_test)\n","imp = model1.feature_importances_\n","score = model1.score(x_test,y_test)\n","print(score)\n","\n","pd.DataFrame(np.column_stack((features,imp)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WZzu-VquAGpg","colab_type":"code","colab":{}},"cell_type":"code","source":["def model_construct1(q_inp_size, r_inp_size, feature_size, mode='train'):\n","  inp1 = tf.keras.layers.Input(shape=q_inp_size)\n","  inp2 = tf.keras.layers.Input(shape=r_inp_size)\n","  inp3 = tf.keras.layers.Input(shape=feature_size)\n","\n","  x3 = tf.keras.layers.Dense(1024,activation='relu')(inp3)\n","  x3 = tf.keras.layers.Dense(512,activation='relu')(x3)\n","  x3 = tf.keras.layers.Dense(256,activation='relu')(x3)\n","  x3 = tf.keras.layers.Dense(128,activation='relu')(x3)\n","  \n","  f1 = tf.keras.layers.Add()([inp1,inp2])\n","  f2 = tf.keras.layers.Multiply()([inp1,inp2])\n","  f3 = tf.keras.layers.Subtract()([inp1,inp2])\n","  \n","  f1 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(f1)\n","  f2 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(f2)\n","  f3 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(f3)\n","  inps1 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(inp1)\n","  inps2 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(inp2)\n","\n","  comb = tf.keras.layers.Concatenate(axis=-1)([inps1,inps2,f1,f2,f3])\n","\n","  x1 = tf.keras.layers.Conv1D(64,5,activation='relu')(comb)\n","  x1 = tf.keras.layers.Conv1D(32,3,activation='relu')(x1)\n","  \n","  x2 = tf.keras.layers.LSTM(32,activation='relu',return_sequences=True)(comb)\n","  x2 = tf.keras.layers.LSTM(32,activation='relu',return_sequences=True)(x2)\n","  \n","  comb = tf.keras.layers.Concatenate(axis=1)([x1,x2])\n","  \n","  x1 = tf.keras.layers.Flatten()(comb)\n","  x1 = tf.keras.layers.Dense(1024,activation='relu')(x1)\n","  x1 = tf.keras.layers.Dense(512,activation='relu')(x1)\n","  x1 = tf.keras.layers.Dense(256,activation='relu')(x1)\n","  x1 = tf.keras.layers.Dense(128,activation='relu')(x1)\n","  \n","  x = tf.keras.layers.Concatenate(axis=-1)([x1,x3])\n","  \n","  x = tf.keras.layers.Dense(512,activation='relu')(x)\n","  x = tf.keras.layers.Dense(256,activation='relu')(x)\n","  out = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n","\n","  model = tf.keras.models.Model(inputs=[inp1,inp2,inp3],outputs=[out],name=\"Embedding Model with Features\")\n","  if mode == 'train':\n","    model.summary()\n","  return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0zjzO0mHvtHg","colab_type":"code","colab":{}},"cell_type":"code","source":["def model_construct2(q_inp_size, r_inp_size, mode='train'):\n","  inp1 = tf.keras.layers.Input(shape=q_inp_size)\n","  inp2 = tf.keras.layers.Input(shape=r_inp_size)\n","\n","  f1 = tf.keras.layers.Add()([inp1,inp2])\n","  f2 = tf.keras.layers.Multiply()([inp1,inp2])\n","  f3 = tf.keras.layers.Subtract()([inp1,inp2])\n","  \n","  f1 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(f1)\n","  f2 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(f2)\n","  f3 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(f3)\n","  inps1 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(inp1)\n","  inps2 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(inp2)\n","\n","  comb = tf.keras.layers.Concatenate(axis=-1)([inps1,inps2,f1,f2,f3])\n","\n","  x1 = tf.keras.layers.Conv1D(64,5,activation='relu')(comb)\n","  x1 = tf.keras.layers.Conv1D(32,3,activation='relu')(x1)\n","  \n","  x2 = tf.keras.layers.LSTM(32,activation='relu',return_sequences=True)(comb)\n","  x2 = tf.keras.layers.LSTM(32,activation='relu',return_sequences=True)(x2)\n","  \n","  comb = tf.keras.layers.Concatenate(axis=1)([x1,x2])\n","  \n","  x1 = tf.keras.layers.Flatten()(comb)\n","  x1 = tf.keras.layers.Dense(1024,activation='relu')(x1)\n","  x1 = tf.keras.layers.Dense(512,activation='relu')(x1)\n","  x1 = tf.keras.layers.Dense(256,activation='relu')(x1)\n","  x1 = tf.keras.layers.Dense(128,activation='relu')(x1)\n","  out = tf.keras.layers.Dense(1,activation='sigmoid')(x1)\n","\n","  model = tf.keras.models.Model(inputs=[inp1,inp2],outputs=[out],name=\"Embedding Model with Features\")\n","  if mode == 'train':\n","    model.summary()\n","  return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xNhdfbIPWZVv","colab_type":"code","colab":{}},"cell_type":"code","source":["def model_construct3(features_shape,mode='train'):\n","  inp = tf.keras.layers.Input(shape=features_shape)\n","  x = tf.keras.layers.Dense(1024, activation='tanh')(inp)\n","  x = tf.keras.layers.Dense(1024, activation='tanh')(x)\n","  x = tf.keras.layers.Dense(1024, activation='tanh')(x)\n","  out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n","\n","  model = tf.keras.models.Model(inputs=[inp],outputs=[out])\n","  if mode=='train':\n","    model.summary()\n","  \n","  return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x0zoRlQ0B2Rf","colab_type":"code","colab":{}},"cell_type":"code","source":["model_name = 'model_3.h5'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DfHPoFO2P7aS","colab_type":"code","outputId":"d273b077-f542-497a-d911-8b7733b65008","executionInfo":{"status":"ok","timestamp":1546107170986,"user_tz":-330,"elapsed":1964606,"user":{"displayName":"Nithish Balachandar","photoUrl":"","userId":"11673492507565107492"}},"colab":{"base_uri":"https://localhost:8080/","height":2145}},"cell_type":"code","source":["keras_model = model_construct1((train_q_embed.shape[1],),(train_r_embed.shape[1],),(len(features),))\n","# keras_model = model_construct2((train_q_embed.shape[1],),(train_r_embed.shape[1],))\n","# keras_model = model_construct3((len(features),))\n","plot_model(keras_model, to_file='./'+model_name.split('.')[0]+'.png', show_shapes=True)\n","tf.keras.backend.clear_session()\n","tpu_model = tf.contrib.tpu.keras_to_tpu_model(keras_model, \n","                                              strategy=tf.contrib.tpu.TPUDistributionStrategy(\n","                                                  tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n","\n","tpu_model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n","tpu_model.fit([train_q_embed,train_r_embed,train_f_data],train_labels,epochs=3,batch_size=128, validation_data=([test_q_embed,test_r_embed,test_f_data],test_labels))\n","# tpu_model.fit([train_q_embed,train_r_embed],train_labels,epochs=3,batch_size=128, validation_data=([test_q_embed,test_r_embed],test_labels))\n","# tpu_model.fit(train_f_data,train_labels,epochs=3,batch_size=128, validation_data=(test_f_data,test_labels))\n","tpu_model.save_weights('./'+model_name, overwrite=True)\n","! cp -fR model_3.h5 './gdrive/My Drive/Microsoft AI/Finalised Version/Models/'\n","! cp -fR model_3.png './gdrive/My Drive/Microsoft AI/Finalised Version/Models/'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 30)           0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, 30)           0                                            \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 30)           0           input_1[0][0]                    \n","                                                                 input_2[0][0]                    \n","__________________________________________________________________________________________________\n","multiply (Multiply)             (None, 30)           0           input_1[0][0]                    \n","                                                                 input_2[0][0]                    \n","__________________________________________________________________________________________________\n","subtract (Subtract)             (None, 30)           0           input_1[0][0]                    \n","                                                                 input_2[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 30, 1)        0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 30, 1)        0           input_2[0][0]                    \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 30, 1)        0           add[0][0]                        \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 30, 1)        0           multiply[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 30, 1)        0           subtract[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 30, 5)        0           lambda_3[0][0]                   \n","                                                                 lambda_4[0][0]                   \n","                                                                 lambda[0][0]                     \n","                                                                 lambda_1[0][0]                   \n","                                                                 lambda_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv1d (Conv1D)                 (None, 26, 64)       1664        concatenate[0][0]                \n","__________________________________________________________________________________________________\n","lstm (LSTM)                     (None, 30, 32)       4864        concatenate[0][0]                \n","__________________________________________________________________________________________________\n","conv1d_1 (Conv1D)               (None, 24, 32)       6176        conv1d[0][0]                     \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   (None, 30, 32)       8320        lstm[0][0]                       \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 54, 32)       0           conv1d_1[0][0]                   \n","                                                                 lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 1728)         0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            (None, 16)           0                                            \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1024)         1770496     flatten[0][0]                    \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 1024)         17408       input_3[0][0]                    \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 512)          524800      dense_4[0][0]                    \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 512)          524800      dense[0][0]                      \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 256)          131328      dense_5[0][0]                    \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 256)          131328      dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","dense_7 (Dense)                 (None, 128)          32896       dense_6[0][0]                    \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 128)          32896       dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 256)          0           dense_7[0][0]                    \n","                                                                 dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dense_8 (Dense)                 (None, 512)          131584      concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","dense_9 (Dense)                 (None, 256)          131328      dense_8[0][0]                    \n","__________________________________________________________________________________________________\n","dense_10 (Dense)                (None, 1)            257         dense_9[0][0]                    \n","==================================================================================================\n","Total params: 3,450,145\n","Trainable params: 3,450,145\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","INFO:tensorflow:Querying Tensorflow master (b'grpc://10.55.107.146:8470') for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 8632142647207248797)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 10477140063745816645)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 10276791914971716669)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1897814302911787561)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 8529200974487477992)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12768773219755683145)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 17958140478545807835)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2446073328760077560)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 9669043179306597211)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 14556954967801482848)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 17089360062284566222)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 16685121306450059563)\n","WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n","Train on 479997 samples, validate on 400000 samples\n","Epoch 1/3\n","INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(16,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(16, 30), dtype=tf.float32, name='input_10'), TensorSpec(shape=(16, 30), dtype=tf.float32, name='input_20'), TensorSpec(shape=(16, 16), dtype=tf.float32, name='input_30'), TensorSpec(shape=(16, 1), dtype=tf.float32, name='dense_10_target_10')]\n","INFO:tensorflow:Overriding default placeholder.\n","INFO:tensorflow:Remapping placeholder for input_1\n","INFO:tensorflow:Remapping placeholder for input_2\n","INFO:tensorflow:Remapping placeholder for input_3\n","INFO:tensorflow:Started compiling\n","INFO:tensorflow:Finished compiling. Time elapsed: 9.250977039337158 secs\n","INFO:tensorflow:Setting weights on TPU model.\n","479744/479997 [============================>.] - ETA: 0s - loss: 0.5969 - acc: 0.6769INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(15,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(15, 30), dtype=tf.float32, name='input_10'), TensorSpec(shape=(15, 30), dtype=tf.float32, name='input_20'), TensorSpec(shape=(15, 16), dtype=tf.float32, name='input_30'), TensorSpec(shape=(15, 1), dtype=tf.float32, name='dense_10_target_10')]\n","INFO:tensorflow:Overriding default placeholder.\n","INFO:tensorflow:Remapping placeholder for input_1\n","INFO:tensorflow:Remapping placeholder for input_2\n","INFO:tensorflow:Remapping placeholder for input_3\n","INFO:tensorflow:Started compiling\n","INFO:tensorflow:Finished compiling. Time elapsed: 9.144530057907104 secs\n","479872/479997 [============================>.] - ETA: 0s - loss: 0.5969 - acc: 0.6769INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(16,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(16, 30), dtype=tf.float32, name='input_10'), TensorSpec(shape=(16, 30), dtype=tf.float32, name='input_20'), TensorSpec(shape=(16, 16), dtype=tf.float32, name='input_30'), TensorSpec(shape=(16, 1), dtype=tf.float32, name='dense_10_target_10')]\n","INFO:tensorflow:Overriding default placeholder.\n","INFO:tensorflow:Remapping placeholder for input_1\n","INFO:tensorflow:Remapping placeholder for input_2\n","INFO:tensorflow:Remapping placeholder for input_3\n","INFO:tensorflow:Started compiling\n","INFO:tensorflow:Finished compiling. Time elapsed: 9.233492136001587 secs\n","479997/479997 [==============================] - 657s 1ms/step - loss: 0.5969 - acc: 0.6769 - val_loss: 0.4231 - val_acc: 0.8576\n","Epoch 2/3\n","479997/479997 [==============================] - 628s 1ms/step - loss: 0.5927 - acc: 0.6806 - val_loss: 0.4315 - val_acc: 0.8597\n","Epoch 3/3\n","479997/479997 [==============================] - 633s 1ms/step - loss: 0.5907 - acc: 0.6823 - val_loss: 0.4253 - val_acc: 0.8695\n","INFO:tensorflow:Copying TPU weights to the CPU\n"],"name":"stdout"}]},{"metadata":{"id":"1_jsR_x3Wg36","colab_type":"text"},"cell_type":"markdown","source":["#Evaluation Code"]},{"metadata":{"id":"t7IXnuLyz8dy","colab_type":"code","colab":{}},"cell_type":"code","source":["data2 = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Finalised Version/All_TestFeatures.csv\",sep=',')\n","qids = pd.read_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/Hashed/Latest Features/Test_QueryID.csv\").values.ravel()\n","x_test_actual = data2[features]\n","find_n_save_prob(qids=qids,\n","                 x_test_actual=x_test_actual,\n","                 mtype=\"nn\",\n","                 fname=model_name)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F6kadv8caPy9","colab_type":"text"},"cell_type":"markdown","source":["# Find MRR"]},{"metadata":{"id":"Ti_gEbGNh5D6","colab_type":"code","colab":{}},"cell_type":"code","source":["model_name = 'model_3.h5'\n","model = model_construct1((test_q_embed.shape[1],),(test_r_embed.shape[1],),(len(features),),mode='eval')\n","# model = model_construct2((test_q_embed.shape[1],),(test_r_embed.shape[1],),mode='eval')\n","# model = model_construct3((len(features),))\n","model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n","model.load_weights(\"./gdrive/My Drive/Microsoft AI/Finalised Version/Models/\"+model_name)\n","probs = model.predict([test_q_embed,test_r_embed,test_f_data]).ravel()\n","# probs = model.predict(test_f_data).ravel()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"azTJakXQADXv","colab_type":"code","outputId":"b847f8bb-d330-4918-8aa1-95627088687d","executionInfo":{"status":"ok","timestamp":1546107425755,"user_tz":-330,"elapsed":2198302,"user":{"displayName":"Nithish Balachandar","photoUrl":"","userId":"11673492507565107492"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["np.count_nonzero(np.round(probs)), probs.shape[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(24001, 400000)"]},"metadata":{"tags":[]},"execution_count":64}]},{"metadata":{"id":"e4O-Rw4ZkAaX","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_MRR_df(probs,test_queryID,test_labelID):\n","  p = []\n","  q = []\n","  for i in range(0,probs.shape[0],10):\n","    l = []\n","    h = []    \n","    l.append(int(test_queryID.ravel()[i]))\n","    h.append(int(test_queryID.ravel()[i]))\n","    \n","    v = probs[i:i+10].T\n","    l.extend(v)\n","    h.append(test_labelID.ravel()[i])\n","    \n","    p.append(l)\n","    q.append(h)\n","\n","  y_pred = pd.DataFrame(p)\n","  y_pred.columns = ['queryID']+list(range(10))\n","  y_true = pd.DataFrame(q)\n","  y_true.columns = ['queryID','labelID']\n","  y_true = y_true.sort_values(by=['queryID']).reset_index(drop=True)\n","  y_pred = y_pred.sort_values(by=['queryID']).reset_index(drop=True)\n","  return y_pred, y_true"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zIYoYblpcpAQ","colab_type":"code","colab":{}},"cell_type":"code","source":["y_pred, y_true = get_MRR_df(probs,test_queryID,test_labelID)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"0xC64Pe2jnsA","colab":{}},"cell_type":"code","source":["def MRR(sub,ref):\n","  scores = []\n","  for i in range(ref.shape[0]):\n","    position = ref['labelID'][i]\n","    rank = np.argsort(sub.iloc[i])[::-1].tolist().index(position)\n","    rank +=1\n","    scores.append(1.0/rank)\n","\n","  score = np.mean(scores)\n","  print(score)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OrtvfHNoBzUe","colab_type":"code","outputId":"530b388a-fcf3-4ee5-8bea-9072b0e5194a","executionInfo":{"status":"ok","timestamp":1546107443366,"user_tz":-330,"elapsed":2213423,"user":{"displayName":"Nithish Balachandar","photoUrl":"","userId":"11673492507565107492"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["MRR(y_pred,y_true)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.33632479347041844\n"],"name":"stdout"}]},{"metadata":{"id":"QzxyBAm8Xfwx","colab_type":"code","colab":{}},"cell_type":"code","source":["def find_n_save_prob(qids,mtype,fname,x_test_actual=None,model=None,eval_gen=None,model_name=None):\n","  if(mtype=='tree'):\n","    assert model!=None,\"Model is None\"\n","    probs = model.predict_proba(x_test_actual)[:,1]\n","  elif(mtype=='simple_nn'):\n","    model = model_construct2(features)\n","    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n","    model.load_weights(\"./gdrive/My Drive/Microsoft AI/Finalised Version/Models/\"+model_name)\n","    probs = model.predict(x_test_actual).ravel()\n","  elif(mtype=='em_nn_whole'):\n","#     model = model_construct1((30,),(30,),(16,),'eval')\n","#     model = model_construct2((30,),(30,),mode='eval')\n","    model = model_construct3((len(features),),mode='eval')\n","    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n","    model.load_weights(\"./gdrive/My Drive/Microsoft AI/Finalised Version/Models/\"+model_name)\n","    probs = model.predict(x_test_actual).ravel()\n","  else:\n","    model = model_construct1((30,),(30,))\n","    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n","    model.load_weights(\"./gdrive/My Drive/Microsoft AI/Finalised Version/Models/\"+model_name)\n","    probs = model.predict_generator(eval_gen).ravel()\n","  \n","  p = []\n","  if(mtype=='em_nn_whole'):\n","    t = x_test_actual[0]\n","  else:\n","    t = x_test_actual\n","  for i in range(0,t.shape[0],10):\n","      l = []\n","      v = probs[i:i+10].T\n","      l.append(qids[i])\n","      l.extend(v)\n","      p.append(l)\n","\n","  result = pd.DataFrame(p)\n","  \n","  try:\n","    os.mkdir(os.path.join(\"gdrive/My Drive/Microsoft AI/Finalised Version/Models/Results\",mtype))\n","  except:\n","    pass\n","  try:\n","    os.mkdir(os.path.join(\"gdrive/My Drive/Microsoft AI/Finalised Version/Models/Results\",mtype,fname))\n","  except:\n","    pass\n","  save_name = os.path.join(\"gdrive/My Drive/Microsoft AI/Finalised Version/Models/Results\",mtype,fname,\"answer.tsv\")\n","  result.to_csv(save_name,sep='\\t',header=None,index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WM3N0PAK0MUQ","colab_type":"code","outputId":"b2cd3003-607d-4395-9baa-770db860f4f8","executionInfo":{"status":"error","timestamp":1546107454287,"user_tz":-330,"elapsed":2215926,"user":{"displayName":"Nithish Balachandar","photoUrl":"","userId":"11673492507565107492"}},"colab":{"base_uri":"https://localhost:8080/","height":554}},"cell_type":"code","source":["qids = pd.read_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/Hashed/Latest Features/Test2_QueryID.csv\").values.ravel()\n","eval_embed = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Finalised Version/Eval2_Embeddings.csv\",sep=',',header=None)\n","eval_q_embed = eval_embed.values[:,1:31]\n","eval_r_embed = eval_embed.values[:,31:]\n","eval_f_data = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Finalised Version/All_TestFeatures_##.csv\",sep=',',usecols=features).values\n","\n","find_n_save_prob(qids=qids,\n","                 mtype=\"em_nn_whole\",\n","                 fname=model_name,\n","                 x_test_actual=eval_f_data,\n","#                  x_test_actual=[eval_q_embed,eval_r_embed],\n","#                  x_test_actual=[eval_q_embed,eval_r_embed,eval_f_data],\n","                 model_name=model_name)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-70-4a873757b3fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#                  x_test_actual=[eval_q_embed,eval_r_embed],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#                  x_test_actual=[eval_q_embed,eval_r_embed,eval_f_data],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                  model_name=model_name)\n\u001b[0m","\u001b[0;32m<ipython-input-69-ae73235e2b01>\u001b[0m in \u001b[0;36mfind_n_save_prob\u001b[0;34m(qids, mtype, fname, x_test_actual, model, eval_gen, model_name)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_construct3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./gdrive/My Drive/Microsoft AI/Finalised Version/Models/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_actual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1544\u001b[0;31m         \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    782\u001b[0m                      \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m                      \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m                      ' layers.')\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m   \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 15 layers into a model with 4 layers."]}]},{"metadata":{"id":"FJLYRGR-frs6","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}