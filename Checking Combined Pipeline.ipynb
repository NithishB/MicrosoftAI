{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Checking Combined Pipeline.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["7kBvPCCLZJ60","28wH_tw3VF9-","wwsi605FWP1H","1_jsR_x3Wg36"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"lFWjUrpBpV1k","colab_type":"code","outputId":"fa1f7a95-3902-4811-8b10-7573d9ce3f1e","executionInfo":{"status":"ok","timestamp":1545989584751,"user_tz":-330,"elapsed":104924,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}},"colab":{"base_uri":"https://localhost:8080/","height":4220}},"cell_type":"code","source":["from __future__ import division\n","\n","import os\n","import sys\n","import time\n","import nltk\n","import math\n","import random\n","import pickle\n","import zipfile\n","import numpy as np\n","import pandas as pd\n","nltk.download('all')\n","import tensorflow as tf\n","from google.colab import drive\n","from collections import Counter\n","from keras.utils import Sequence\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem.porter import PorterStemmer\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from sklearn.metrics.pairwise import cosine_similarity\n","from keras.optimizers import Adam, Adamax, Nadam, Adagrad\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"hxeqsOevAW6X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"55c5f24b-12be-4d3c-83bf-614b2ddb6484","executionInfo":{"status":"ok","timestamp":1545990373452,"user_tz":-330,"elapsed":893615,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["drive.mount(\"/content/gdrive\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"ELPdv9bbDftm","colab_type":"code","colab":{}},"cell_type":"code","source":["TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9X8V2-sHN6M4","colab_type":"text"},"cell_type":"markdown","source":["# Function Definitions"]},{"metadata":{"id":"h_eYYwGMrir3","colab_type":"code","colab":{}},"cell_type":"code","source":["porter = PorterStemmer()\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wdtNjKX6skrU","colab_type":"code","colab":{}},"cell_type":"code","source":["def clean(df,ltype,dtype):\n","  column = 'query'\n","  #Lowercase conversion\n","  df[column] = df[column].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n","  print(column+\": Converted to lowercase\")\n","  #Tokenization\n","  df[column] = df[column].apply(word_tokenize)\n","  print(column+\": Tokenized\")\n","  #Removing Stop Words\n","  #df[column] = [w for w in word_tokens if not w in stop_words]\n","  #df[column] = df[column].apply(lambda x: [item for item in x if item not in stop_words])\n","  df[column] = df[column].apply(lambda x: \" \".join(word for word in x if word not in stop_words))\n","  print(column+\": StopWords Removed\")\n","  #Lemmatization - root words\n","  df[column] = df[column].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()]))\n","  print(column+\": Root words Lemmatized\")\n","  #Split a-b into a and b\n","  df[column] = df[column].str.replace('-',' ')\n","  print(column+\": - Replaced\")\n","  #Removing punctuations\n","  df[column] = df[column].str.replace('[^\\w\\s]','')\n","  print(column+\": Removed punctions \")\n","  #Replacing numbers\n","  df[column] = df[column].str.replace('[0-9]','#')\n","  print(column+\": Replaced Numbers \")\n","  #Stemming\n","  #df[column] = df[column].apply(lambda x: \" \".join([porter.stem(word) for word in x.split()]))\n","  #print(column+\": Stemming done\")\n","  \n","  column = 'response'\n","  #Lowercase conversion\n","  df[column] = df[column].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n","  print(column+\": Converted to lowercase\")\n","  #Tokenization\n","  df[column] = df[column].apply(word_tokenize)\n","  print(column+\": Tokenized\")\n","  #Removing Stop Words\n","  #df[column] = [w for w in word_tokens if not w in stop_words]\n","  #df[column] = df[column].apply(lambda x: [item for item in x if item not in stop_words])\n","  df[column] = df[column].apply(lambda x: \" \".join(word for word in x if word not in stop_words))\n","  print(column+\": StopWords Removed\")\n","  #Lemmatization - root words\n","  df[column] = df[column].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()]))\n","  print(column+\": Root words Lemmatized\")\n","  #Split a-b into a and b\n","  df[column] = df[column].str.replace('-',' ')\n","  print(column+\": - Replaced\")\n","  #Removing punctuations\n","  df[column] = df[column].str.replace('[^\\w\\s]','')\n","  print(column+\": Removed punctions \")\n","  #Replacing numbers\n","  df[column] = df[column].str.replace('[0-9]+','#')\n","  print(column+\": Replaced Numbers \")\n","  #Stemming\n","  #df[column] = df[column].apply(lambda x: \" \".join([porter.stem(word) for word in x.split()]))\n","  #print(column+\": Stemming done\")\n","  df['Row_id'] = df.index\n","  print(df.head())\n","  return (df)#df.to_csv(\"./gdrive/My Drive/Microsoft AI/Junk/Sample Data/clean_\"+ltype+\"_latest_#.csv\",sep=',',index=None)\n","\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"AYxU700Ew-y1","colab_type":"code","colab":{}},"cell_type":"code","source":["def avg_word(sentence):\n","  words = sentence.split()\n","  return (sum(len(word) for word in words)/len(words))\n","\n","\n","def get_basic_features(df,ltype,dtype):\n","  \n","  column='query'\n","  df[column[0]+'_word_count'] = df[column].apply(lambda x: len(str(x).split(\" \")))\n","  print(column+\"Word Count Done\")\n","  df[column[0]+'_char_count'] = df[column].str.len()\n","  print(column+\"Char Count Done\")\n","  df[column[0]+'_avg_word'] = df[column].apply(lambda x: avg_word(x))\n","  print(column+\"Avg Word Length Done\")\n","  \n","  column='response'\n","  df[column[0]+'_word_count'] = df[column].apply(lambda x: len(str(x).split(\" \")))\n","  print(column+\"Word Count Done\")\n","  df[column[0]+'_char_count'] = df[column].str.len()\n","  print(column+\"Char Count Done\")\n","  df[column[0]+'_avg_word'] = df[column].apply(lambda x: avg_word(x))\n","  print(column+\"Avg Word Length Done\")\n","  print(df.shape)\n","  print(df.head())\n","  df.to_csv(\"./gdrive/My Drive/Microsoft AI/Junk/Sample Data/\"+ltype+\"_Basic_features.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pVvJEIa2vU1f","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_cosine_sim(df,ltype,dtype):\n","  cosine_sim = []\n","  for i in range(0,df.shape[0],10):\n","    if(i%10000 == 0):\n","      print(i)\n","    sent = (df['query'][i], *(df['response'][i:i+10].T))\n","    text = [t for t in sent]\n","    vectorizer = TfidfVectorizer()\n","    vect = vectorizer.fit_transform(text).toarray()\n","    vectors = [t for t in vect]\n","    cosine_sim.extend(cosine_similarity(vectors)[0,1:].ravel())\n","  cosine_sim = pd.Series(cosine_sim).rename(\"Cosine\")\n","  cosine_sim.columns = ['Cosine']\n","  print(cosine_sim.shape)\n","  print(cosine_sim.head())\n","  cosine_sim.to_csv(\"./gdrive/My Drive/Microsoft AI/Junk/Sample Data/\"+ltype+\"_Cosine_Similarity.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VE0iyD27xKdJ","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_jaccard_sim(df,ltype,dtype):\n","  jac_sim = []\n","  for i in range(df.shape[0]):\n","    if(i%10000 == 0):\n","      print(i)\n","    a = set(df['query'][i].split())\n","    b = set(df['response'][i].split())\n","    c = a.intersection(b)\n","    jac_sim.append(float(len(c)) / (len(a) + len(b) - len(c)))\n","  jac_sim = pd.Series(jac_sim).rename(\"Jaccard\")\n","  jac_sim.columns = ['Jaccard']\n","  print(jac_sim.shape)\n","  print(jac_sim.head())\n","  jac_sim.to_csv(\"./gdrive/My Drive/Microsoft AI/Junk/Sample Data/\"+ltype+\"_Jaccard_Similarity.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5JEZuE7kyjYQ","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_rouge_score(df,ltype,dtype):\n","  r1_recall = []\n","  r1_precision = []\n","  for i in range(df.shape[0]):\n","    if(i%10000 == 0):\n","      print(i)\n","    a = set(df['query'][i].split())\n","    b = set(df['response'][i].split())\n","    c = a.intersection(b)\n","    r1_recall.append(float (len(c)/len(a)))\n","    r1_precision.append(float (len(c)/len(b)))\n","  rouge_pd = pd.concat([pd.Series(r1_recall),pd.Series(r1_precision)],axis=1)\n","  rouge_pd.columns = ['Rouge R','Rouge P']\n","  print(rouge_pd.shape)\n","  print(rouge_pd.head())\n","  rouge_pd.to_csv(\"./gdrive/My Drive/Microsoft AI/Junk/Sample Data/\"+ltype+\"_Rouge_Score.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yUcx2q05zOhL","colab_type":"code","colab":{}},"cell_type":"code","source":["def TFIDFMatrix(docs):\n","  terms = []\n","  for i in range(len(docs)):\n","    words = docs[i].split()\n","    for w in words:\n","      if w not in terms:\n","        terms.append(w)\n","  \n","  tf = np.ndarray((len(terms),len(docs)))\n","  \n","  for i in range(len(terms)):\n","    for j in range(len(docs)):\n","      tf[i][j] = docs[j].split().count(terms[i])\n","      \n","  idf = np.array([np.log(1 + (float(len(docs))/np.count_nonzero(tf[i]))) for i in range(len(terms))])\n","  tf = 0.5 + 0.5*tf/tf.max(axis=0)  \n","  tfidf = np.array([tf[i]*idf[i] for i in range(len(terms))]).T\n","  return tfidf, terms\n","\n","def get_tfidf_score(df,ltype,dtype):\n","  scores = []\n","  for i in range(0,df.shape[0],10):\n","    \n","    if(i%10000 == 0):\n","      print(i)\n","    \n","    docs = df['response'][i:i+10].tolist()\n","    mat, terms = TFIDFMatrix(docs)\n","    mat = pd.DataFrame(mat, columns=terms)\n","    \n","    words = df['query'][i].split()\n","    tf = {}\n","    for w in words:\n","      if w not in tf.keys():\n","        tf[w] = 1\n","      else:\n","        tf[w] += 1\n","    mx = max(tf.values())\n","    new_row = np.zeros(mat.shape[1])\n","    for k in tf.keys():\n","      tf[k] = 0.5+0.5*tf[k]/mx\n","      try:\n","        new_row[mat.columns.tolist().index(k)] = tf[k]\n","      except:\n","        pass\n","    mat = pd.DataFrame(np.vstack((new_row,mat.values)),columns=terms)\n","    scores.extend(cosine_similarity(mat.values)[0,1:])\n","  scores = pd.Series(scores).rename(\"Tfidf\")\n","  scores.columns = ['Tfidf']\n","  print(scores.shape)\n","  print(scores.head())\n","  scores.to_csv(\"./gdrive/My Drive/Microsoft AI/Junk/Sample Data/\"+ltype+\"_Tfidf_Score.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Lira57cq0OiA","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_bleu_score(df,ltype,dtype):\n","  bleu_score = []\n","  for i in range(df.shape[0]):\n","    if(i%10000 == 0):\n","      print(i)\n","    bleu_score.append([sentence_bleu([df['response'][i].split()],df['query'][i].split(),weights=(1,0)), sentence_bleu([df['response'][i].split()],df['query'][i].split(),weights=(1,0))])\n","  bleu_score = pd.DataFrame(bleu_score)\n","  bleu_score.columns = ['BLEU 1','BLEU 2']\n","  print(bleu_score.shape)\n","  print(bleu_score.head())\n","  bleu_score.to_csv(\"./gdrive/My Drive/Microsoft AI/Junk/Sample Data/\"+ltype+\"_BLEU_Score.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cSiv5GF1XlhM","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_bm25_score(ltype,dtype):\n","  bm_data = pd.read_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\"/Latest Features/\"+ltype+\"_BM25.tsv\",sep='\\t',header=None)\n","  bm = []\n","  for i in range(len(bm_data)):\n","    bm.extend(bm_data.iloc[i,1:].tolist())\n","  bm = pd.Series(bm).rename(\"BM25\")[:20]\n","  bm.columns = ['BM25']\n","  print(bm.shape)\n","  print(bm.head())\n","  bm.to_csv(\"./gdrive/My Drive/Microsoft AI/Junk/Sample Data/\"+ltype+\"_BM25.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vu0CKHKWjH_z","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_dssm_score(ltype,dtype):\n","  with open(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\"/Latest Features/DSSM/\"+ltype+\"_dssm.txt\") as f:\n","    dvalues = f.readlines()\n","  with open(\"./gdrive/My Drive/Microsoft AI/Finalised Version/\"+dtype+\"/Latest Features/DSSM/\"+ltype+\"_cdssm.txt\") as f:\n","    cvalues = f.readlines()\n","  dvalues = [float(val.split(\"\\n\")[0]) for val in dvalues]\n","  cvalues = [float(val.split(\"\\n\")[0]) for val in cvalues]\n","  if(ltype=='Train'):\n","    dssm = pd.concat([pd.Series(dvalues[1:]),pd.Series(cvalues[1:])],axis=1)[:20]\n","  else:\n","    dssm = pd.concat([pd.Series(dvalues),pd.Series(cvalues)],axis=1)[:20]\n","  dssm.columns = ['DSSM','CDSSM']\n","  print(dssm.shape)\n","  print(dssm.head())\n","  dssm.to_csv(\"./gdrive/My Drive/Microsoft AI/Junk/Sample Data/\"+ltype+\"_DSSM.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"41c3kXlQoOS5","colab_type":"text"},"cell_type":"markdown","source":["# Data Loading"]},{"metadata":{"id":"JRW1G_Br1HtF","colab_type":"code","colab":{}},"cell_type":"code","source":["data = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Junk/Sample Data/Sample_Raw_Data.csv\",sep=',')[:20]\n","data.columns = ['queryID','query','response','label','labelID']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cW6xqQCwsHSP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":503},"outputId":"c1818153-82cf-4544-a22e-baa3b307e003","executionInfo":{"status":"ok","timestamp":1545995014338,"user_tz":-330,"elapsed":962,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["data = clean(data,\"Ha\",\"Ho\")\n","data.shape"],"execution_count":74,"outputs":[{"output_type":"stream","text":["query: Converted to lowercase\n","query: Tokenized\n","query: StopWords Removed\n","query: Root words Lemmatized\n","query: - Replaced\n","query: Removed punctions \n","query: Replaced Numbers \n","response: Converted to lowercase\n","response: Tokenized\n","response: StopWords Removed\n","response: Root words Lemmatized\n","response: - Replaced\n","response: Removed punctions \n","response: Replaced Numbers \n","   queryID          query                                           response  \\\n","0      131   corporation   company incorporated specific nation  often wi...   \n","1      131   corporation   today  growing community # certified b corp # ...   \n","2      131   corporation   corporation definition  association individual...   \n","3      131   corporation   example corporation sentence  # work consultan...   \n","4      131   corporation   #  government owned corporation  utility railr...   \n","\n","   label  labelID  Row_id  \n","0      0        0       0  \n","1      0        1       1  \n","2      0        2       2  \n","3      0        3       3  \n","4      0        4       4  \n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(20, 6)"]},"metadata":{"tags":[]},"execution_count":74}]},{"metadata":{"id":"uQGWgMLgZ0_P","colab_type":"code","colab":{}},"cell_type":"code","source":["data2 = pd.read_csv(\"./gdrive/My Drive/Microsoft AI/Junk/Sample Data/Sample_Raw_Test_Data.csv\",sep=',')[:20]\n","data2.columns = ['queryID','query','response','labelID']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FVgdJHsDg9J0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":503},"outputId":"0cf10be9-fb94-41b7-c064-d520f5d8fbca","executionInfo":{"status":"ok","timestamp":1545995577024,"user_tz":-330,"elapsed":968,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["data2 = clean(data2,\"Ha\",\"Ho\")\n","data2.shape"],"execution_count":99,"outputs":[{"output_type":"stream","text":["query: Converted to lowercase\n","query: Tokenized\n","query: StopWords Removed\n","query: Root words Lemmatized\n","query: - Replaced\n","query: Removed punctions \n","query: Replaced Numbers \n","response: Converted to lowercase\n","response: Tokenized\n","response: StopWords Removed\n","response: Root words Lemmatized\n","response: - Replaced\n","response: Removed punctions \n","response: Replaced Numbers \n","   queryID                           query  \\\n","0  1135787  distance erie buffalo new york   \n","1  1135787  distance erie buffalo new york   \n","2  1135787  distance erie buffalo new york   \n","3  1135787  distance erie buffalo new york   \n","4  1135787  distance erie buffalo new york   \n","\n","                                            response  labelID  Row_id  \n","0  erie canal distance table erie canal longest c...        0       0  \n","1  distance erie buffalo  distance erie buffalo s...        1       1  \n","2  distance erie buffalo straight line # mile # k...        2       2  \n","3  erie canal distance  erie canal distance table...        3       3  \n","4  erie s metropolitan area consists approximatel...        4       4  \n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(20, 5)"]},"metadata":{"tags":[]},"execution_count":99}]},{"metadata":{"id":"7kBvPCCLZJ60","colab_type":"text"},"cell_type":"markdown","source":["## Train Data Features Extraction"]},{"metadata":{"id":"xS_js-5bonEV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"851be17a-b097-4878-c691-92b4d2a389bf","executionInfo":{"status":"ok","timestamp":1545995049580,"user_tz":-330,"elapsed":888,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_cosine_sim(data,'Train','Hashed')"],"execution_count":75,"outputs":[{"output_type":"stream","text":["0\n","(20,)\n","0    0.280419\n","1    0.000000\n","2    0.116886\n","3    0.374302\n","4    0.284388\n","Name: Cosine, dtype: float64\n"],"name":"stdout"}]},{"metadata":{"id":"ehLNuh8go6Vb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"71f16d12-260a-47eb-daa7-d927f7241831","executionInfo":{"status":"ok","timestamp":1545995050013,"user_tz":-330,"elapsed":520,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_jaccard_sim(data,'Train','Hashed')"],"execution_count":76,"outputs":[{"output_type":"stream","text":["0\n","(20,)\n","0    0.035714\n","1    0.000000\n","2    0.066667\n","3    0.062500\n","4    0.047619\n","Name: Jaccard, dtype: float64\n"],"name":"stdout"}]},{"metadata":{"id":"KGlxMv3do_r7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"4545d232-7949-4403-d200-d13c3f75c576","executionInfo":{"status":"ok","timestamp":1545995051070,"user_tz":-330,"elapsed":927,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_rouge_score(data,'Train','Hashed')"],"execution_count":77,"outputs":[{"output_type":"stream","text":["0\n","(20, 2)\n","   Rouge R   Rouge P\n","0      1.0  0.035714\n","1      0.0  0.000000\n","2      1.0  0.066667\n","3      1.0  0.062500\n","4      1.0  0.047619\n"],"name":"stdout"}]},{"metadata":{"id":"9ONaaf2bpCRb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"2e5e9666-886c-465f-c1db-4be5c4f2bf02","executionInfo":{"status":"ok","timestamp":1545995063722,"user_tz":-330,"elapsed":566,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_tfidf_score(data,'Train','Hashed')"],"execution_count":78,"outputs":[{"output_type":"stream","text":["0\n","(20,)\n","0    0.061623\n","1    0.031652\n","2    0.046495\n","3    0.062919\n","4    0.062085\n","Name: Tfidf, dtype: float64\n"],"name":"stdout"}]},{"metadata":{"id":"imIeAcjipEFa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"7e6e50ed-7aee-4b4a-aa24-d4fde45b0144","executionInfo":{"status":"ok","timestamp":1545995065069,"user_tz":-330,"elapsed":509,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_bleu_score(data,'Train','Hashed')"],"execution_count":79,"outputs":[{"output_type":"stream","text":["0\n","(20, 2)\n","         BLEU 1        BLEU 2\n","0  8.533048e-17  8.533048e-17\n","1  0.000000e+00  0.000000e+00\n","2  4.139938e-08  4.139938e-08\n","3  1.522998e-08  1.522998e-08\n","4  3.775135e-11  3.775135e-11\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"}]},{"metadata":{"id":"G6QqS_Z_Zu2k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"2413afe8-ec5e-4b06-ebd8-0df228f2d76c","executionInfo":{"status":"ok","timestamp":1545995201539,"user_tz":-330,"elapsed":134630,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_bm25_score(\"Train\",\"Hashed\")"],"execution_count":80,"outputs":[{"output_type":"stream","text":["(20,)\n","0     0.000000\n","1    10.204230\n","2     0.000000\n","3     7.216898\n","4    10.952685\n","Name: BM25, dtype: float64\n"],"name":"stdout"}]},{"metadata":{"id":"XrADQN3qmiGd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"4f6e8e91-8f72-4c1b-acb3-f1fc2e74c0a3","executionInfo":{"status":"ok","timestamp":1545995209290,"user_tz":-330,"elapsed":139260,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_dssm_score('Train','Hashed')"],"execution_count":81,"outputs":[{"output_type":"stream","text":["(20, 2)\n","       DSSM     SDSSM\n","0  0.452772  0.346784\n","1  0.506993  0.182843\n","2  0.184675  0.336138\n","3  0.481602  0.227964\n","4  0.418071  0.266236\n"],"name":"stdout"}]},{"metadata":{"id":"bU6I31IvysJ_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"720c8835-251d-423d-b49e-e0d446682637","executionInfo":{"status":"ok","timestamp":1545995209299,"user_tz":-330,"elapsed":137630,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_basic_features(data,'Train','Hashed')"],"execution_count":82,"outputs":[{"output_type":"stream","text":["queryWord Count Done\n","queryChar Count Done\n","queryAvg Word Length Done\n","responseWord Count Done\n","responseChar Count Done\n","responseAvg Word Length Done\n","(20, 12)\n","   queryID          query                                           response  \\\n","0      131   corporation   company incorporated specific nation  often wi...   \n","1      131   corporation   today  growing community # certified b corp # ...   \n","2      131   corporation   corporation definition  association individual...   \n","3      131   corporation   example corporation sentence  # work consultan...   \n","4      131   corporation   #  government owned corporation  utility railr...   \n","\n","   label  labelID  Row_id  q_word_count  q_char_count  q_avg_word  \\\n","0      0        0       0             3            13        11.0   \n","1      0        1       1             3            13        11.0   \n","2      0        2       2             3            13        11.0   \n","3      0        3       3             3            13        11.0   \n","4      0        4       4             3            13        11.0   \n","\n","   r_word_count  r_char_count  r_avg_word  \n","0            48           315    7.052632  \n","1            25           146    5.545455  \n","2            24           163    7.777778  \n","3            22           138    6.157895  \n","4            32           208    7.080000  \n"],"name":"stdout"}]},{"metadata":{"id":"ZeF54hjQZVJ4","colab_type":"text"},"cell_type":"markdown","source":["## Test Data Features Extraction"]},{"metadata":{"id":"5VXwQLxlZs3n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"6151a65b-1592-405b-d6c3-7a74c28bb296","executionInfo":{"status":"ok","timestamp":1545995625890,"user_tz":-330,"elapsed":953,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_cosine_sim(data2,'Test','Hashed')"],"execution_count":100,"outputs":[{"output_type":"stream","text":["0\n","(20,)\n","0    0.228288\n","1    0.316879\n","2    0.229560\n","3    0.251271\n","4    0.198103\n","Name: Cosine, dtype: float64\n"],"name":"stdout"}]},{"metadata":{"id":"oUkrIUNZZiXY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"d86de5dd-273e-4376-9084-b2cf93d8028a","executionInfo":{"status":"ok","timestamp":1545995625895,"user_tz":-330,"elapsed":576,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_jaccard_sim(data2,'Test','Hashed')"],"execution_count":101,"outputs":[{"output_type":"stream","text":["0\n","(20,)\n","0    0.172414\n","1    0.130435\n","2    0.130435\n","3    0.142857\n","4    0.097561\n","Name: Jaccard, dtype: float64\n"],"name":"stdout"}]},{"metadata":{"id":"ECUU47GzZmLE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"1da104ab-1604-4c4b-a54c-10b2ffa2dab2","executionInfo":{"status":"ok","timestamp":1545995626272,"user_tz":-330,"elapsed":625,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_rouge_score(data2,'Test','Hashed')"],"execution_count":102,"outputs":[{"output_type":"stream","text":["0\n","(20, 2)\n","   Rouge R   Rouge P\n","0      1.0  0.172414\n","1      0.6  0.142857\n","2      0.6  0.142857\n","3      1.0  0.142857\n","4      0.8  0.100000\n"],"name":"stdout"}]},{"metadata":{"id":"RwMfbXtOZo18","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"7a427a0a-fa73-4eef-de94-59ef6b50be6a","executionInfo":{"status":"ok","timestamp":1545995626648,"user_tz":-330,"elapsed":621,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_tfidf_score(data2,'Test','Hashed')"],"execution_count":103,"outputs":[{"output_type":"stream","text":["0\n","(20,)\n","0    0.094306\n","1    0.111116\n","2    0.109972\n","3    0.095189\n","4    0.095038\n","Name: Tfidf, dtype: float64\n"],"name":"stdout"}]},{"metadata":{"id":"uxwPLKXxZk4a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":225},"outputId":"3b29d59b-ff4d-4993-aa5a-53d82419a28f","executionInfo":{"status":"ok","timestamp":1545995627114,"user_tz":-330,"elapsed":713,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_bleu_score(data2,'Test','Hashed')"],"execution_count":104,"outputs":[{"output_type":"stream","text":["0\n","(20, 2)\n","     BLEU 1    BLEU 2\n","0  0.000611  0.000611\n","1  0.004043  0.004043\n","2  0.007366  0.007366\n","3  0.000030  0.000030\n","4  0.000180  0.000180\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"}]},{"metadata":{"id":"j9NXajMVhqcp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"8438c21c-9bf3-43ae-9a08-5a272d0a7405","executionInfo":{"status":"ok","timestamp":1545995630559,"user_tz":-330,"elapsed":3723,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_bm25_score(\"Test\",\"Hashed\")"],"execution_count":105,"outputs":[{"output_type":"stream","text":["(20,)\n","0     0.000000\n","1    35.142888\n","2    31.517606\n","3    33.750318\n","4    25.981487\n","Name: BM25, dtype: float64\n"],"name":"stdout"}]},{"metadata":{"id":"dfbEuMBImo21","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"3df6ec08-9dd0-40a4-dbf9-dc25aa92c2a4","executionInfo":{"status":"ok","timestamp":1545995631656,"user_tz":-330,"elapsed":4319,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_dssm_score('Test','Hashed')"],"execution_count":106,"outputs":[{"output_type":"stream","text":["(20, 2)\n","       DSSM     CDSSM\n","0  0.314036  0.263362\n","1  0.760360  0.549321\n","2  0.677732  0.546766\n","3  0.589114  0.464406\n","4  0.398033  0.260062\n"],"name":"stdout"}]},{"metadata":{"id":"Cfy3Ux3G1Gcf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":607},"outputId":"500bb229-e20e-47b5-b272-cbadcc3b5f38","executionInfo":{"status":"ok","timestamp":1545995695385,"user_tz":-330,"elapsed":704,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["get_basic_features(data2,\"Test\",\"Hashed\")"],"execution_count":108,"outputs":[{"output_type":"stream","text":["queryWord Count Done\n","queryChar Count Done\n","queryAvg Word Length Done\n","responseWord Count Done\n","responseChar Count Done\n","responseAvg Word Length Done\n","(20, 11)\n","   queryID                           query  \\\n","0  1135787  distance erie buffalo new york   \n","1  1135787  distance erie buffalo new york   \n","2  1135787  distance erie buffalo new york   \n","3  1135787  distance erie buffalo new york   \n","4  1135787  distance erie buffalo new york   \n","\n","                                            response  labelID  Row_id  \\\n","0  erie canal distance table erie canal longest c...        0       0   \n","1  distance erie buffalo  distance erie buffalo s...        1       1   \n","2  distance erie buffalo straight line # mile # k...        2       2   \n","3  erie canal distance  erie canal distance table...        3       3   \n","4  erie s metropolitan area consists approximatel...        4       4   \n","\n","   q_word_count  q_char_count  q_avg_word  r_word_count  r_char_count  \\\n","0             5            30         5.2            51           231   \n","1             5            30         5.2            36           210   \n","2             5            30         5.2            32           187   \n","3             5            30         5.2            71           337   \n","4             5            30         5.2            59           370   \n","\n","   r_avg_word  \n","0    4.309524  \n","1    5.833333  \n","2    5.777778  \n","3    4.684211  \n","4    6.638298  \n"],"name":"stdout"}]},{"metadata":{"id":"3xVLu-nQiEpU","colab_type":"text"},"cell_type":"markdown","source":["## Merge Features"]},{"metadata":{"id":"6PZCltFJoGWv","colab_type":"code","colab":{}},"cell_type":"code","source":["def merge_features(data,ltype, dtype):\n","  path = os.path.join(\"gdrive/My Drive/Microsoft AI/Junk/Sample Data/\",ltype+\"_\")\n","  \n","  #s1 = pd.read_csv(path+\"QueryID.csv\",sep=',',header=None).reset_index(drop=True)\n","  s2 = pd.read_csv(path+\"Cosine_Similarity.csv\",sep=',',header=None).reset_index(drop=True)\n","  s3 = pd.read_csv(path+\"Jaccard_Similarity.csv\",sep=',',header=None).reset_index(drop=True)\n","  s4 = pd.read_csv(path+\"BM25.csv\",sep=',',header=None).reset_index(drop=True)\n","  s5 = pd.read_csv(path+\"Tfidf_Score.csv\",sep=',',header=None).reset_index(drop=True)\n","  s6 = pd.read_csv(path+\"Rouge_Score.csv\",sep=',').reset_index(drop=True)\n","  s7 = pd.read_csv(path+\"BLEU_Score.csv\",sep=',').reset_index(drop=True)\n","  s8 = pd.read_csv(path+\"DSSM.csv\",sep=',').reset_index(drop=True)\n","  if(ltype == 'Train'):\n","    #s9 = data['label'].reset_index(drop=True)\n","    s10 = pd.read_csv(path+\"Basic_features.csv\",sep=',',low_memory=False).drop(columns=['query','response']).reset_index(drop=True)\n","    all_feat = pd.concat([s2,s3,s4,s5,s6,s7,s8,s10],axis=1)\n","    all_feat.columns =  ['Cosine','Jaccard','BM25','Tfidf','Rouge P','Rouge R','BLEU 1','BLEU 2','DSSM','CDSSM','queryID','label','labelID','Row_id','q_word_count','q_char_count','q_avg_word','r_word_count','r_char_count','r_avg_word']\n","    print(all_feat.shape)\n","    print(all_feat.head())\n","    all_feat.to_csv(\"gdrive/My Drive/Microsoft AI/Junk/Sample Data/All_\"+ltype+\"Features.csv\",sep=',',index=None)\n","  else:\n","    s9 = pd.read_csv(path+\"Basic_features.csv\",sep=',',low_memory=False).drop(columns=['query','response']).reset_index(drop=True)\n","    all_feat = pd.concat([s2,s3,s4,s5,s6,s7,s8,s9],axis=1)\n","    all_feat.columns =  ['Cosine','Jaccard','BM25','Tfidf','Rouge P','Rouge R','BLEU 1','BLEU 2','DSSM','CDSSM','queryID','labelID','Row_id','q_word_count','q_char_count','q_avg_word','r_word_count','r_char_count','r_avg_word']\n","    print(all_feat.shape)\n","    print(all_feat.head())\n","    all_feat.to_csv(\"gdrive/My Drive/Microsoft AI/Junk/Sample Data/All_\"+ltype+\"Features.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1AlbX_O0xl6y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":503},"outputId":"86fbb0af-c9d5-483c-a416-cd75b18f98e8","executionInfo":{"status":"ok","timestamp":1545995428436,"user_tz":-330,"elapsed":580,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["merge_features(data,'Train','Hashed')"],"execution_count":87,"outputs":[{"output_type":"stream","text":["(20, 20)\n","     Cosine   Jaccard       BM25     Tfidf  Rouge P   Rouge R        BLEU 1  \\\n","0  0.280419  0.035714   0.000000  0.061623      1.0  0.035714  8.533048e-17   \n","1  0.000000  0.000000  10.204230  0.031652      0.0  0.000000  0.000000e+00   \n","2  0.116886  0.066667   0.000000  0.046495      1.0  0.066667  4.139938e-08   \n","3  0.374302  0.062500   7.216898  0.062919      1.0  0.062500  1.522998e-08   \n","4  0.284388  0.047619  10.952685  0.062085      1.0  0.047619  3.775135e-11   \n","\n","         BLEU 2      DSSM     CDSSM  queryID  label  labelID  Row_id  \\\n","0  8.533048e-17  0.452772  0.346784      131      0        0       0   \n","1  0.000000e+00  0.506993  0.182843      131      0        1       1   \n","2  4.139938e-08  0.184675  0.336138      131      0        2       2   \n","3  1.522998e-08  0.481602  0.227964      131      0        3       3   \n","4  3.775135e-11  0.418071  0.266236      131      0        4       4   \n","\n","   q_word_count  q_char_count  q_avg_word  r_word_count  r_char_count  \\\n","0             3            13        11.0            48           315   \n","1             3            13        11.0            25           146   \n","2             3            13        11.0            24           163   \n","3             3            13        11.0            22           138   \n","4             3            13        11.0            32           208   \n","\n","   r_avg_word  \n","0    7.052632  \n","1    5.545455  \n","2    7.777778  \n","3    6.157895  \n","4    7.080000  \n"],"name":"stdout"}]},{"metadata":{"id":"27NjEGcz519T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":382},"outputId":"dc832455-b92f-4473-fa58-3cceda55572f","executionInfo":{"status":"ok","timestamp":1545995704171,"user_tz":-330,"elapsed":710,"user":{"displayName":"Sharansundar S","photoUrl":"","userId":"01755706745962493316"}}},"cell_type":"code","source":["merge_features(data2,\"Test\",\"Hashed\")"],"execution_count":109,"outputs":[{"output_type":"stream","text":["(20, 19)\n","     Cosine   Jaccard       BM25     Tfidf  Rouge P   Rouge R    BLEU 1  \\\n","0  0.228288  0.172414   0.000000  0.094306      1.0  0.172414  0.000611   \n","1  0.316879  0.130435  35.142888  0.111116      0.6  0.142857  0.004043   \n","2  0.229560  0.130435  31.517606  0.109972      0.6  0.142857  0.007366   \n","3  0.251271  0.142857  33.750318  0.095189      1.0  0.142857  0.000030   \n","4  0.198103  0.097561  25.981487  0.095038      0.8  0.100000  0.000180   \n","\n","     BLEU 2      DSSM     CDSSM  queryID  labelID  Row_id  q_word_count  \\\n","0  0.000611  0.314036  0.263362  1135787        0       0             5   \n","1  0.004043  0.760360  0.549321  1135787        1       1             5   \n","2  0.007366  0.677732  0.546766  1135787        2       2             5   \n","3  0.000030  0.589114  0.464406  1135787        3       3             5   \n","4  0.000180  0.398033  0.260062  1135787        4       4             5   \n","\n","   q_char_count  q_avg_word  r_word_count  r_char_count  r_avg_word  \n","0            30         5.2            51           231    4.309524  \n","1            30         5.2            36           210    5.833333  \n","2            30         5.2            32           187    5.777778  \n","3            30         5.2            71           337    4.684211  \n","4            30         5.2            59           370    6.638298  \n"],"name":"stdout"}]},{"metadata":{"id":"28wH_tw3VF9-","colab_type":"text"},"cell_type":"markdown","source":["# Model and Hyper parameters"]},{"metadata":{"id":"ONUOQXZEW4qo","colab_type":"code","colab":{}},"cell_type":"code","source":["features = ['Cosine','Jaccard','BM25','Tfidf','Rouge P','Rouge R','BLEU 1','BLEU 2','DSSM','CDSSM',\n","            'q_word_count','q_char_count','q_avg_word','r_word_count','r_char_count','r_avg_word']\n","\n","embed = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Junk/Just For now/Train_Embeddings.csv\")\n","data = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Finalised Version/All_TrainFeatures.csv\",usecols=features).iloc[:embed.shape[0]]\n","labelID = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Finalised Version/Hashed/Latest Features/Train_labelID.csv\",sep=',',header=None).iloc[:embed.shape[0]]\n","queryID = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Finalised Version/Hashed/Latest Features/Train_QueryID.csv\",sep=',',header=None).iloc[:embed.shape[0]]\n","combined_data = pd.concat([data,embed],axis=1)\n","\n","split = int(10*((embed.shape[0]*0.7)//10))\n","print(split)\n","\n","train_data, test_data = combined_data.iloc[:split].reset_index( drop = True), combined_data.iloc[split:].reset_index( drop = True)\n","train_labelID, test_labelID = labelID.iloc[:split].reset_index( drop = True), labelID.iloc[split:].reset_index( drop = True)\n","train_queryID, test_queryID = queryID.iloc[:split].reset_index( drop = True), queryID.iloc[split:].reset_index( drop = True)\n","\n","tr_ind = train_data[train_data['labels']==1].index\n","fl_ind = train_data[train_data['labels']==0].index\n","\n","tr_data, fl_data = train_data.iloc[tr_ind].reset_index( drop = True), train_data.iloc[fl_ind].reset_index( drop = True)\n","\n","tr_ind = random.sample(range(0,tr_data.shape[0]),tr_data.shape[0])\n","fl_ind = random.sample(range(0,fl_data.shape[0]),tr_data.shape[0]*4)\n","\n","train_data = pd.concat([tr_data.iloc[tr_ind],fl_data.iloc[fl_ind]],axis=0).sample(frac=1).reset_index( drop = True)\n","\n","train_f_data, test_f_data = train_data.values[:,:16], test_data.values[:,:16]\n","train_q_embed, test_q_embed = train_data.values[:,46:76], test_data.values[:,46:76]\n","train_r_embed, test_r_embed = train_data.values[:,16:46], test_data.values[:,16:46]\n","train_labels, test_labels = train_data.values[:,-1], test_data.values[:,-1]\n","\n","print(train_q_embed.shape)\n","print(train_r_embed.shape)\n","print(train_f_data.shape)\n","print(test_q_embed.shape)\n","print(test_r_embed.shape)\n","print(test_f_data.shape)\n","print(train_labels.shape)\n","print(test_labels.shape)\n","print(test_labelID.shape)\n","print(test_queryID.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"s_irxGBszoK8","colab_type":"code","colab":{}},"cell_type":"code","source":["features = ['Cosine','Jaccard','BM25','Tfidf','Rouge P','Rouge R','BLEU 1','BLEU 2','DSSM','CDSSM',\n","            'q_word_count','q_char_count','q_avg_word','r_word_count','r_char_count','r_avg_word']\n","\n","data = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Finalised Version/All_TrainFeatures.csv\")\n","split = int(10*((data.shape[0]*0.7)//10))\n","print(split)\n","\n","train_data, test_data = data.iloc[:split]\n","\n","tr_data = data[data['label'] == 1].reset_index(drop=True)\n","fl_data = data[data['label'] == 0].reset_index(drop=True)\n","tr_ind = random.sample(range(0,tr_data.shape[0]),tr_data.shape[0])\n","fl_ind = random.sample(range(0,fl_data.shape[0]),tr_data.shape[0]*3)\n","df = pd.concat([tr_data.iloc[tr_ind],fl_data.iloc[fl_ind]],axis=0).sample(frac=1).reset_index( drop = True)\n","\n","y = df['label']\n","x = df[features]\n","print(len(features))\n","\n","# x = (x - x.min()) / (x.max() - x.min())\n","x = (x - df.mean()) / df.std()\n","x_train, x_test = x.values[:split,:], x.values[split:,:]\n","y_train, y_test = y.values[:split], y.values[split:]\n","x_train.shape, y_train.shape, x_test.shape, y_test.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wwsi605FWP1H","colab_type":"text"},"cell_type":"markdown","source":["## Hyper Parameter Tuning"]},{"metadata":{"id":"0cHU2sbzVlvX","colab_type":"code","colab":{}},"cell_type":"code","source":["hyperparams = {\n","    \"trees\":{\n","        \"criterions\" : [\"gini\",\"entropy\"],\n","        \"max_depth\" : [3,4,5,6,7,8,9,10],\n","        \"max_features\": [\"log2\",\"sqrt\",\"auto\"], \n","        \"n_estimators\": [20,40,60,80,100]}, \n","    \"gbc\":{\n","        \"loss\" : [\"deviance\",\"exponential\"],\n","        \"learning_rate\" : [0.1,0.5,0.8,1.0],\n","        \"n_estimators\" : [20,40,60,80,100],\n","        \"max_features\" : [\"log2\",\"sqrt\",\"auto\"],\n","        \"max_depth\" : [3,4,5,6,7,8,9,10]},\n","    \"nn\":{\n","        \"learning_rate\" : [0.1,0.01,0.05,0.001,0.005],\n","        \"optim\" : [\"Adam\",\"Adamax\",\"Adagrad\",\"Nadam\"], \n","        \"activation\": [\"tanh\",\"sigmoid\",\"relu\",\"linear\"]}\n","}\n","\n","results = {}\n","results['RFC'] = {}\n","\n","for cr in hyperparams[\"trees\"][\"criterions\"]:\n","    results['RFC'][str(cr)] = {}\n","    for dep in hyperparams[\"trees\"][\"max_depth\"]:\n","        results['RFC'][str(cr)][str(dep)] = {}\n","        for feat in hyperparams[\"trees\"][\"max_features\"]:\n","            results['RFC'][str(cr)][str(dep)][str(feat)] = {}\n","            for est in hyperparams[\"trees\"][\"n_estimators\"]:\n","                results['RFC'][str(cr)][str(dep)][str(feat)][str(est)] = {}\n","                print(\"Hyperparameters used\")\n","                print(\"Criterion : \",cr)\n","                print(\"Maximum Depth : \",dep)\n","                print(\"Maximum Features : \",feat)\n","                print(\"Number of Estimators : \",est)\n","                model = RandomForestClassifier(n_estimators=est,\n","                                              criterion=cr,\n","                                              max_features=feat,\n","                                              max_depth=dep,\n","                                              verbose=3,\n","                                              class_weight=\"balanced\")\n","                \n","                model.fit(x_train,y_train)\n","                y_pred = model.predict(x_test)\n","                imp = model.feature_importances_\n","                score = model.score(x_test,y_test)\n","                print(score)\n","                results['RFC'][str(cr)][str(dep)][str(feat)][str(est)][\"score\"] = score\n","                \n","                f = pd.DataFrame(np.column_stack((features,imp)))\n","                f.columns = ['feature','importance']\n","                print(f.sort_values(by=['importance'],ascending=False))\n","                results['RFC'][str(cr)][str(dep)][str(feat)][str(est)][\"f_imp\"] = f\n","\n","results['ETC'] = {}\n","\n","for cr in hyperparams[\"trees\"][\"criterions\"]:\n","    results['ETC'][str(cr)] = {}\n","    for dep in hyperparams[\"trees\"][\"max_depth\"]:\n","        results['ETC'][str(cr)][str(dep)] = {}\n","        for feat in hyperparams[\"trees\"][\"max_features\"]:\n","            results['ETC'][str(cr)][str(dep)][str(feat)] = {}\n","            for est in hyperparams[\"trees\"][\"n_estimators\"]:\n","                results['ETC'][str(cr)][str(dep)][str(feat)][str(est)] = {}\n","                print(\"Hyperparameters used\")\n","                print(\"Criterion : \",cr)\n","                print(\"Maximum Depth : \",dep)\n","                print(\"Maximum Features : \",feat)\n","                print(\"Number of Estimators : \",est)\n","                model = ExtraTreesClassifier(n_estimators=est,\n","                                              criterion=cr,\n","                                              max_features=feat,\n","                                              max_depth=dep,\n","                                              verbose=3,\n","                                              warm_start=True,\n","                                              class_weight=\"balanced\")\n","                \n","                model.fit(x_train,y_train)\n","                y_pred = model.predict(x_test)\n","                imp = model.feature_importances_\n","                score = model.score(x_test,y_test)\n","                print(score)\n","                results['ETC'][str(cr)][str(dep)][str(feat)][str(est)][\"score\"] = score\n","                \n","                f = pd.DataFrame(np.column_stack((features,imp)))\n","                f.columns = ['feature','importance']\n","                print(f.sort_values(by=['importance'],ascending=False))\n","                results['ETC'][str(cr)][str(dep)][str(feat)][str(est)][\"f_imp\"] = f\n","\n","results['GBC'] = {}\n","\n","for ls in hyperparams[\"gbc\"][\"loss\"]:\n","    results['GBC'][str(ls)] = {}\n","    for lr in hyperparams[\"gbc\"][\"learning_rate\"]:\n","        results['GBC'][str(ls)][str(lr)] = {}\n","        for est in hyperparams[\"gbc\"][\"n_estimators\"]:\n","            results['GBC'][str(ls)][str(lr)][str(est)] = {}\n","            for feat in hyperparams[\"gbc\"][\"max_features\"]:\n","                results['GBC'][str(ls)][str(lr)][str(est)][str(feat)] = {}\n","                for dep in hyperparams[\"gbc\"][\"max_depth\"]:\n","                    results['GBC'][str(ls)][str(lr)][str(est)][str(feat)][str(dep)] = {}\n","                    print(\"Hyperparameters used\")\n","                    print(\"Loss : \",ls)\n","                    print(\"Learning rate : \",lr)\n","                    print(\"Number of Estimators : \",est)\n","                    print(\"Maximum Features : \",feat)\n","                    print(\"Maximum Depth : \",dep)\n","                    model = GradientBoostingClassifier(loss=ls,\n","                                                       learning_rate=lr,\n","                                                       n_estimators=est,\n","                                                       max_features=feat,\n","                                                       max_depth=dep,\n","                                                       verbose=3)\n","\n","                    model.fit(x_train,y_train)\n","                    y_pred = model.predict(x_test)\n","                    imp = model.feature_importances_\n","                    score = model.score(x_test,y_test)\n","                    print(score)\n","                    results['GBC'][str(ls)][str(lr)][str(est)][str(feat)][str(dep)][\"score\"] = score\n","\n","                    f = pd.DataFrame(np.column_stack((features,imp)))\n","                    f.columns = ['feature','importance']\n","                    print(f.sort_values(by=['importance'],ascending=False))\n","                    results['GBC'][str(ls)][str(lr)][str(est)][str(feat)][str(dep)][\"f_imp\"] = f\n","\n","# tree models tabulation\n","columns = [\"model type\",\"criterion\",\"learning rate\",\"max features\",\"max depth\",\"# of estimators\",\"score\"]\n","l = []\n","for mdl in results.keys():\n","    if(mdl != \"GBC\"):\n","        for crt in results[mdl].keys():\n","            for mfeat in results[mdl][crt].keys():\n","                for mdep in results[mdl][crt][mfeat].keys():\n","                    for nest in results[mdl][crt][mfeat][mdep].keys():\n","                        score = results[mdl][crt][mfeat][mdep][nest][\"score\"]\n","                        l.append([mdl,crt,None,mfeat,mdep,nest,score])\n","    else:\n","        for crt in results[mdl].keys():\n","            for lr in results[mdl][crt].keys():\n","                for nest in results[mdl][crt][lr].keys():\n","                    for mfeat in results[mdl][crt][lr][nest].keys():\n","                        for mdep in results[mdl][crt][lr][nest][mfeat].keys():\n","                            score = results[mdl][crt][lr][nest][mfeat][mdep][\"score\"]\n","                            l.append([mdl,crt,lr,mfeat,mdep,nest,score])\n","\n","res = pd.DataFrame(l,columns=columns)\n","res = res.sort_values(by=['score'],ascending=False).reset_index(drop=True)\n","\n","res.to_csv(\"./Tree_Hyper.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jLXLWx_4WVYj","colab_type":"code","colab":{}},"cell_type":"code","source":["results['nn'] = {}\n","for optim in hyperparams['nn']['optim']:\n","    results['nn'][str(optim)] = {}\n","    for lr in hyperparams['nn']['learning_rate']:\n","        results['nn'][str(optim)][str(lr)] = {}\n","        for act in hyperparams['nn']['activation']:\n","            print(\"Hyperparameter used\")\n","            print(\"Optimizer : \",optim)\n","            print(\"Learning rate : \",lr)\n","            print(\"Activation : \",act)\n","            results['nn'][str(optim)][str(lr)][str(act)] = {}\n","            inp = L.Input(shape=(len(features),))\n","            x = L.Dense(512, activation=act)(inp)\n","            x = L.Dense(256, activation=act)(x)\n","            x = L.Dense(128, activation=act)(x)\n","            x = L.Dense(128, activation=act)(x)\n","            x = L.Dense(64, activation=act)(x)\n","            out = L.Dense(1, activation=\"sigmoid\")(x)\n","            \n","            model = M.Model(inputs=[inp],outputs=[out])\n","            model.summary()\n","            if optim == \"Adam\":\n","                o = Adam(lr)\n","            elif optim == \"Adagrad\":\n","                o = Adagrad(lr)\n","            elif optim == \"Adamax\":\n","                o = Adamax(lr)\n","            elif optim == \"Nadam\":\n","                o = Nadam(lr)\n","            model.compile(o,loss='binary_crossentropy',metrics=['accuracy'])\n","            \n","            model.fit(x_train,y_train,epochs=10)\n","            score = model.evaluate(x_test,y_test)\n","            y_pred = model.predict(x_test)\n","            \n","            results['nn'][str(optim)][str(lr)][str(act)][\"score\"] = score\n","\n","# tree models tabulation\n","columns = [\"optimizer\",\"learning rate\",\"activation\",\"score\"]\n","l = []\n","for mdl in results.keys():\n","    if(mdl == \"nn\"):\n","        for optim in results[mdl].keys():\n","            for lr in results[mdl][optim].keys():\n","                for act in results[mdl][optim][lr].keys():\n","                    score = results[mdl][optim][lr][act][\"score\"]\n","                    l.append([optim,lr,act,score])\n","\n","res = pd.DataFrame(l,columns=columns)\n","res[\"loss\"] = [s[0] for s in res[\"score\"]]\n","res[\"score\"] = [s[1] for s in res[\"score\"]]\n","\n","res = res.sort_values(by=['score'],ascending=False).reset_index(drop=True)\n","res.to_csv(\"NN_Hyper.csv\",sep=',',index=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GQiyTHLBWTZC","colab_type":"text"},"cell_type":"markdown","source":["## Model Training"]},{"metadata":{"id":"SleStFzdVibe","colab_type":"code","colab":{}},"cell_type":"code","source":["#model type,criterion,learning rate,max features,max depth,# of estimators,score\n","#GBC,exponential,0.1,log2,7,100,0.900060606060606\n","#RFC,gini,,10,auto,80,0.6947878787878788\n","ls = \"exponential\"\n","lr = 0.1\n","est = 100\n","feat = \"log2\"\n","dep = 7\n","\n","# model1 = RandomForestClassifier(n_estimators=est,criterion=cr,max_features=feat,max_depth=dep,verbose=3,class_weight=\"balanced_subsample\")\n","model1 = GradientBoostingClassifier(loss=ls, learning_rate=lr,n_estimators=est,max_features=feat,max_depth=dep,verbose=3)\n","# model1 = ExtraTreesClassifier(n_estimators=100, criterion='gini', verbose=0,n_jobs=-1, max_features='auto', min_samples_split=3, min_samples_leaf=20)\n","\n","model1.fit(x_train,y_train)\n","\n","y_pred = model1.predict(x_test)\n","imp = model1.feature_importances_\n","score = model1.score(x_test,y_test)\n","print(score)\n","\n","pd.DataFrame(np.column_stack((features,imp)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WZzu-VquAGpg","colab_type":"code","colab":{}},"cell_type":"code","source":["def model_construct1(q_inp_size, r_inp_size, feature_size, mode='train'):\n","  inp1 = tf.keras.layers.Input(shape=q_inp_size)\n","  inp2 = tf.keras.layers.Input(shape=r_inp_size)\n","  inp3 = tf.keras.layers.Input(shape=feature_size)\n","\n","  x3 = tf.keras.layers.Dense(1024,activation='relu')(inp3)\n","  x3 = tf.keras.layers.Dense(512,activation='relu')(x3)\n","  x3 = tf.keras.layers.Dense(256,activation='relu')(x3)\n","  x3 = tf.keras.layers.Dense(128,activation='relu')(x3)\n","  \n","  f1 = tf.keras.layers.Add()([inp1,inp2])\n","  f2 = tf.keras.layers.Multiply()([inp1,inp2])\n","  f3 = tf.keras.layers.Subtract()([inp1,inp2])\n","  \n","  f1 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(f1)\n","  f2 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(f2)\n","  f3 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(f3)\n","  inps1 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(inp1)\n","  inps2 = tf.keras.layers.Lambda(lambda x: tf.keras.backend.expand_dims(x,axis=-1))(inp2)\n","\n","  comb = tf.keras.layers.Concatenate(axis=-1)([inps1,inps2,f1,f2,f3])\n","\n","  x1 = tf.keras.layers.Conv1D(64,5,activation='relu')(comb)\n","  x1 = tf.keras.layers.Conv1D(32,3,activation='relu')(x1)\n","  \n","  x2 = tf.keras.layers.GRU(32,activation='relu',return_sequences=True)(comb)\n","  x2 = tf.keras.layers.GRU(32,activation='relu',return_sequences=True)(x2)\n","  \n","  comb = tf.keras.layers.Concatenate(axis=1)([x1,x2])\n","  \n","  x1 = tf.keras.layers.Flatten()(comb)\n","  x1 = tf.keras.layers.Dense(1024,activation='relu')(x1)\n","  x1 = tf.keras.layers.Dense(512,activation='relu')(x1)\n","  x1 = tf.keras.layers.Dense(256,activation='relu')(x1)\n","  x1 = tf.keras.layers.Dense(128,activation='relu')(x1)\n","  \n","  x = tf.keras.layers.Concatenate(axis=-1)([x1,x3])\n","  \n","  x = tf.keras.layers.Dense(512,activation='relu')(x)\n","  x = tf.keras.layers.Dense(256,activation='relu')(x)\n","  out = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n","\n","  model = tf.keras.models.Model(inputs=[inp1,inp2,inp3],outputs=[out],name=\"Embedding Model with Features\")\n","  if mode == 'train':\n","    model.summary()\n","  return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xNhdfbIPWZVv","colab_type":"code","colab":{}},"cell_type":"code","source":["def model_construct2(features):\n","  inp = tf.keras.layers.Input(shape=(len(features),))\n","  x = tf.keras.layers.Dense(256, activation='sigmoid')(inp)\n","  x = tf.keras.layers.Dense(512, activation='sigmoid')(x)\n","  x = tf.keras.layers.Dense(1024, activation='sigmoid')(x)\n","  out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n","\n","  model = tf.keras.models.Model(inputs=[inp],outputs=[out])\n","  model.summary()\n","\n","  return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x0zoRlQ0B2Rf","colab_type":"code","colab":{}},"cell_type":"code","source":["keras_model = model_construct1((train_q_embed.shape[1],),(train_r_embed.shape[1],),(len(features),))\n","tf.keras.backend.clear_session()\n","tpu_model = tf.contrib.tpu.keras_to_tpu_model(keras_model, \n","                                              strategy=tf.contrib.tpu.TPUDistributionStrategy(\n","                                                  tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n","\n","tpu_model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n","tpu_model.fit([train_q_embed,train_r_embed,train_f_data],train_labels,epochs=10,batch_size=128,\n","              validation_data=([test_q_embed,test_r_embed,test_f_data],test_labels))\n","\n","tpu_model.save_weights('./features_embeddings_bi-model.h5', overwrite=True)\n","! cp -fR \"./features_embeddings_bi-model.h5\" \"./gdrive/My Drive/Microsoft AI/Junk/\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"1_jsR_x3Wg36","colab_type":"text"},"cell_type":"markdown","source":["# Evaluation Code"]},{"metadata":{"id":"QzxyBAm8Xfwx","colab_type":"code","colab":{}},"cell_type":"code","source":["def find_n_save_prob(qids,mtype,fname,x_test_actual=None,model=None,eval_gen=None):\n","  if(mtype=='tree'):\n","    assert model!=None,\"Model is None\"\n","    probs = model.predict_proba(x_test_actual)[:,1]\n","  elif(mtype=='simple_nn'):\n","    model = model_construct2(features)\n","    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n","    model.load_weights(\"./gdrive/My Drive/Microsoft AI/Junk/features_embeddings_bi-model.h5\")\n","    probs = model.predict(x_test_actual).ravel()\n","  elif(mtype=='em_nn_whole'):\n","    model = model_construct1((30,),(30,),(16,),'eval')\n","    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n","    model.load_weights(\"./gdrive/My Drive/Microsoft AI/Junk/features_embeddings_bi-model.h5\")\n","    probs = model.predict(x_test_actual).ravel()\n","  else:\n","    model = model_construct1((30,),(30,))\n","    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n","    model.load_weights(\"./gdrive/My Drive/Microsoft AI/Junk/features_embeddings_bi-model.h5\")\n","    probs = model.predict_generator(eval_gen).ravel()\n","  \n","  p = []\n","  if(mtype=='em_nn_whole'):\n","    t = x_test_actual[0]\n","  else:\n","    t = x_test_actual\n","  for i in range(0,t.shape[0],10):\n","      l = []\n","      v = probs[i:i+10].T\n","      l.append(qids[i])\n","      l.extend(v)\n","      p.append(l)\n","\n","  result = pd.DataFrame(p)\n","  \n","  try:\n","    os.mkdir(os.path.join(\"gdrive/My Drive/Microsoft AI/Results\",mtype))\n","  except:\n","    pass\n","  try:\n","    os.mkdir(os.path.join(\"gdrive/My Drive/Microsoft AI/Results\",mtype,fname))\n","  except:\n","    pass\n","  save_name = os.path.join(\"gdrive/My Drive/Microsoft AI/Results\",mtype,fname,\"answer.tsv\")\n","  result.to_csv(save_name,sep='\\t',header=None,index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t7IXnuLyz8dy","colab_type":"code","colab":{}},"cell_type":"code","source":["data2 = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Finalised Version/All_TestFeatures.csv\",sep=',')\n","qids = pd.read_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/Hashed/Latest Features/Test_QueryID.csv\").values.ravel()\n","x_test_actual = data2[features]\n","find_n_save_prob(qids,x_test_actual,\"nn\",\"26-12_NN_with_all_feat_norm\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WM3N0PAK0MUQ","colab_type":"code","colab":{}},"cell_type":"code","source":["qids = pd.read_csv(\"./gdrive/My Drive/Microsoft AI/Finalised Version/Hashed/Latest Features/Test_QueryID.csv\").values.ravel()\n","eval_q_embed = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Junk/Just For now/Test_Query_Embeddings.csv\")\n","eval_r_embed = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Junk/Just For now/Test_Response_Embeddings.csv\")\n","eval_f_data = pd.read_csv(\"gdrive/My Drive/Microsoft AI/Finalised Version/All_TestFeatures.csv\",sep=',',usecols=features)\n","find_n_save_prob(qids,\"em_nn_whole\",\"Features_Embedding_BiDir_Based\",x_test_actual=[eval_q_embed,eval_r_embed,eval_f_data])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ti_gEbGNh5D6","colab_type":"code","colab":{}},"cell_type":"code","source":["model = model_construct1((30,),(30,),(16,),'eval')\n","model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n","model.load_weights(\"./gdrive/My Drive/Microsoft AI/Junk/features_embeddings_bi-model.h5\")\n","probs = model.predict([test_q_embed,test_r_embed,test_f_data]).ravel()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e4O-Rw4ZkAaX","colab_type":"code","colab":{}},"cell_type":"code","source":["p = []\n","for i in range(0,probs.shape[0],10):\n","  l = []\n","  v = probs[i:i+10].T\n","  l.append(test_queryID.values[i][0])\n","  l.extend(v)\n","  p.append(l)\n","\n","p = np.array(p)\n","y_pred = pd.DataFrame(p[:,1:],index=p[:,0].astype(int))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BQRM2jLMt-6D","colab_type":"code","colab":{}},"cell_type":"code","source":["tmp=pd.DataFrame(test_labelID.values.ravel(),index=test_queryID.astype(int).values.ravel()).reset_index(drop=True)\n","h = []\n","for i in range(0,tmp.shape[0],10):\n","  h.append(tmp.values[i])\n","y_true = pd.DataFrame(h)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2ldtgI6uBe5p","colab_type":"text"},"cell_type":"markdown","source":["### MRR Score"]},{"metadata":{"id":"Agcvda3DFY7e","colab_type":"code","colab":{}},"cell_type":"code","source":["y_pred.columns = list(range(10))\n","y_true.columns = ['labelID']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Sfc-N2hiBhO7","colab_type":"code","colab":{}},"cell_type":"code","source":["def MRR(sub,ref):\n","  scores = []\n","  for q_id in ref.index:\n","    position = ref.iloc[q_id].values.ravel()\n","    #print(position)\n","    rank = np.argsort(sub.iloc[q_id])[::-1].tolist().index(position)\n","    rank +=1\n","    scores.append(1.0/rank)\n","\n","  score = np.mean(scores)\n","  print(score)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OrtvfHNoBzUe","colab_type":"code","colab":{}},"cell_type":"code","source":["MRR(y_pred,y_true)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sqW1FqHnJ63c","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}